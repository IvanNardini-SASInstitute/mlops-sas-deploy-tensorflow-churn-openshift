{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "This notebook is just for settings the demo. \n",
    "\n",
    "It covers the following tasks:\n",
    "\n",
    "    1. Make train, test, and val directories\n",
    "    2. Split raw data in train, test and val data\n",
    "    3. Test impute missing function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "## General\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import pprint\n",
    "\n",
    "# Preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "import apache_beam.io.iobase\n",
    "import apache_beam as beam\n",
    "import tensorflow_transform.beam as tft_beam\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata, schema_utils, dataset_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "\n",
    "BASE_DIR_PATH = os.getcwd()\n",
    "DATA_DIR_PATH = os.path.join(BASE_DIR_PATH, '../data')\n",
    "\n",
    "RAW_DATA_PATH = os.path.join(DATA_DIR_PATH, 'raw/raw_data.csv')\n",
    "TEST_DATA_PATH = os.path.join(DATA_DIR_PATH, 'test_settings_data.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Make train, test, and val directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_directories(datapath):\n",
    "    \n",
    "    train_dir = os.path.join(datapath, 'train')\n",
    "    test_dir = os.path.join(datapath, 'test')\n",
    "    eval_dir = os.path.join(datapath, 'val')\n",
    "    \n",
    "    data_dir_paths = [train_dir, test_dir, eval_dir]\n",
    "    \n",
    "    for data_dir_path in data_dir_paths:\n",
    "        # Check if exist and remove\n",
    "        if os.path.exists(data_dir_path) and os.path.isdir(data_dir_path):\n",
    "            shutil.rmtree(data_dir_path)\n",
    "        # Make dirs\n",
    "        os.mkdir(data_dir_path)\n",
    "        \n",
    "    return data_dir_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Split raw data in training, test and val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_raw_train_test(raw_data_path, data_dir_paths):\n",
    "    \n",
    "    raw_df = pd.read_csv(raw_data_path)\n",
    "    train, test = train_test_split(raw_df, test_size=0.1, random_state = 8)\n",
    "    train, val = train_test_split(train, test_size=0.1, random_state = 8)\n",
    "    \n",
    "    dataframes = [train, test, val]\n",
    "    data_file_names = ['train.csv', 'test.csv', 'val.csv']\n",
    "    \n",
    "    for dataframe, data_dir_path, data_file_name in zip(dataframes, data_dir_paths, data_file_names):\n",
    "        dataframe.to_csv(os.path.join(data_dir_path, data_file_name), index=False)\n",
    "    \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR_PATHS = make_data_directories(DATA_DIR_PATH)\n",
    "split_raw_train_test(RAW_DATA_PATH, DATA_DIR_PATHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Define features and configures feature columns\n",
    "\n",
    "In order to import our training data into TensorFlow, we need to specify what type of data each feature contains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   a       4 non-null      float64\n",
      " 1   b       4 non-null      float64\n",
      " 2   c       5 non-null      object \n",
      " 3   d       5 non-null      int64  \n",
      "dtypes: float64(2), int64(1), object(1)\n",
      "memory usage: 288.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "# Define a dataframe for testing. \n",
    "\n",
    "# We have:\n",
    "\n",
    "# 1. missing values\n",
    "# 2. normalize data\n",
    "\n",
    "\n",
    "data = pd.DataFrame(dict(\n",
    "    a=[1.1, 2.2, 3.3, 4.4, np.NaN], # continuous\n",
    "    b=[5.5, 6.6, 7.7, np.NaN, 0.0], # continous\n",
    "    c=['a', 'b', '', 'a', ''], #categorical\n",
    "    d=[1, 2, 2, 1, 2] #target\n",
    "))\n",
    "\n",
    "data.info()\n",
    "\n",
    "data.to_csv(TEST_DATA_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = ['d']\n",
    "CATEGORICAL_VARIABLES = ['c']\n",
    "NUMERICAL_VARIABLES = ['a', 'b']\n",
    "\n",
    "feature_columns = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_mean_parameter(train_data: pd.DataFrame, column: str) -> float:\n",
    "    ''' Given a column, calculate mean'''\n",
    "    mean = train_data[column].mean()\n",
    "    return mean\n",
    "\n",
    "def _get_impute_parameters(train_data: pd.DataFrame, numerical_features: list) -> dict:\n",
    "    '''For each column in the numerical features, calculate mean.'''\n",
    "\n",
    "    impute_parameters = {}\n",
    "    \n",
    "    for column in numerical_features:\n",
    "        impute_parameters[column] = _get_mean_parameter(train_data, column)\n",
    "        \n",
    "    return impute_parameters\n",
    "\n",
    "def _impute_missing(inputs: dict) -> dict:\n",
    "    impute_parameters = _get_impute_parameters(data, NUMERICAL_VARIABLES)\n",
    "    # Since we modify just some features, \n",
    "    # we need to start by setting `outputs` to a copy of `inputs.\n",
    "    output = inputs.copy()\n",
    "    for key, value in impute_parameters.items():\n",
    "        is_miss = tf.math.is_nan(inputs[key])\n",
    "        tf_mean = tf.constant(value, dtype=np.float64)\n",
    "        output[key] = tf.where(is_miss, tf_mean, inputs[key])\n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CSV files as dataset and Impute missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(dataframe: pd.DataFrame) -> dict:\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(dict(dataframe))\n",
    "    dataset = dataset.map(_impute_missing)\n",
    "    dataset = dataset.repeat(3).shuffle(buffer_size=5, seed=8).batch(5).prefetch(1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice I use Dataset.from_tensor_slices method because data fits into memory. \n",
    "# Also it works on dictionaries, allowing this data to be easily imported.\n",
    "\n",
    "# def input_fn(dataframe: pd.DataFrame) -> dict:\n",
    "#     '''input_fn to read the data and impute missings'''\n",
    "    \n",
    "#     #Extract\n",
    "#     dataframe = _set_categorical_type(dataframe)\n",
    "#     dataframe = _set_categorical_empty(dataframe)\n",
    "#     dataframe = _set_numerical_type(dataframe)\n",
    "#     dataset = tf.data.Dataset.from_tensor_slices(dict(dataframe))\n",
    "    \n",
    "#     #Transform\n",
    "#     dataset = dataset.map(_impute_missing_categorical)\n",
    "#     dataset = dataset.map(_impute_missing_numerical)\n",
    "#     batch_dataset = dataset.repeat(3).shuffle(buffer_size=480, seed=8).batch(5).prefetch(1)\n",
    "    \n",
    "#     #Load : to check\n",
    "#     return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': <tf.Tensor: shape=(5,), dtype=float64, numpy=array([1.1 , 3.3 , 1.1 , 2.2 , 2.75])>, 'b': <tf.Tensor: shape=(5,), dtype=float64, numpy=array([5.5, 7.7, 5.5, 6.6, 0. ])>, 'c': <tf.Tensor: shape=(5,), dtype=string, numpy=array([b'a', b'', b'a', b'b', b''], dtype=object)>, 'd': <tf.Tensor: shape=(5,), dtype=int64, numpy=array([1, 2, 1, 2, 2])>}\n",
      "{'a': <tf.Tensor: shape=(5,), dtype=float64, numpy=array([2.2 , 1.1 , 2.75, 4.4 , 2.2 ])>, 'b': <tf.Tensor: shape=(5,), dtype=float64, numpy=array([6.6 , 5.5 , 0.  , 4.95, 6.6 ])>, 'c': <tf.Tensor: shape=(5,), dtype=string, numpy=array([b'b', b'a', b'', b'a', b'b'], dtype=object)>, 'd': <tf.Tensor: shape=(5,), dtype=int64, numpy=array([2, 1, 2, 1, 2])>}\n",
      "{'a': <tf.Tensor: shape=(5,), dtype=float64, numpy=array([2.75, 3.3 , 4.4 , 3.3 , 4.4 ])>, 'b': <tf.Tensor: shape=(5,), dtype=float64, numpy=array([0.  , 7.7 , 4.95, 7.7 , 4.95])>, 'c': <tf.Tensor: shape=(5,), dtype=string, numpy=array([b'', b'', b'a', b'', b'a'], dtype=object)>, 'd': <tf.Tensor: shape=(5,), dtype=int64, numpy=array([2, 2, 1, 2, 1])>}\n"
     ]
    }
   ],
   "source": [
    "for item in input_fn(data):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in input_fn(data_train).take(1):\n",
    "#     print(item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
