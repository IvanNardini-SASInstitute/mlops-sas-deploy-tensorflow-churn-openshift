{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Scoring Business Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "This notebook provides an example of **how to train Tensorflow classifiers using the HMEQ dataset**\n",
    "\n",
    "The goal is to **predict whether a customer is a BAD (default) borrower**, which in this dataset is a binary classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumption\n",
    "\n",
    "**We are working in big data context**. \n",
    "\n",
    "Then, I'm going to work with HMEQ dataset as it is so large that it would not fit in RAM. \n",
    "\n",
    "Then **we use the Tensorflow framework to deal with that**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports and setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#General\n",
    "import os\n",
    "import functools\n",
    "import shutil\n",
    "import datetime\n",
    "\n",
    "#Analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, confusion_matrix\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Modelling\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, DenseFeatures, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "#Experiment tracking\n",
    "import mlflow\n",
    "mlflow.set_tracking_uri('http://mlflow:5000')\n",
    "mlflow.set_experiment('Credit Churn')\n",
    "\n",
    "# Settings\n",
    "matplotlib.rcParams['figure.figsize'] = (12, 10)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR_PATH = os.getcwd()\n",
    "DATA_DIR_PATH = os.path.join(BASE_DIR_PATH, '../data')\n",
    "\n",
    "# Data directories paths\n",
    "TRAIN_DIR_PATH = os.path.join(DATA_DIR_PATH, 'train')\n",
    "TEST_DIR_PATH = os.path.join(DATA_DIR_PATH, 'test')\n",
    "\n",
    "# Data file paths\n",
    "TRAIN_DATA_PATH = os.path.join(TRAIN_DIR_PATH, 'train.csv')\n",
    "TEST_DATA_PATH = os.path.join(TEST_DIR_PATH, 'test.csv')\n",
    "\n",
    "# Model directories\n",
    "LOGS_DIR = os.path.join(BASE_DIR_PATH, '../logs')\n",
    "MODELS_DIR = os.path.join(BASE_DIR_PATH, '../models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Helpers\n",
    "\n",
    "**Notice**: TF adds tf.keras.layers.experimental.preprocessing to deal with preprocessing tasks\n",
    "\n",
    "TF needs several preprocessing functions in order to perform data manipulation and feature engineering tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing data\n",
    "\n",
    "def _set_categorical_type(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Set the categorical type as string if neeeded\n",
    "    :param dataframe: \n",
    "    :return: dataframe\n",
    "    '''\n",
    "    for column in CATEGORICAL_VARIABLES:\n",
    "        if (dataframe[column].dtype == 'O'):\n",
    "            dataframe[column] = dataframe[column].astype('string')\n",
    "    return dataframe\n",
    "\n",
    "def _set_categorical_empty(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Change object type for categorical variable to avoid TF issue\n",
    "    :param dataframe: \n",
    "    :return: dataframe\n",
    "    '''\n",
    "    for column in CATEGORICAL_VARIABLES:\n",
    "        if any(dataframe[column].isna()):\n",
    "            dataframe[column] = dataframe[column].fillna('')\n",
    "    return dataframe\n",
    "\n",
    "def _set_numerical_type(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Set the numerical type as float64 if needed\n",
    "    :param dataframe:\n",
    "    :return: dataframe\n",
    "    '''\n",
    "    for column in NUMERICAL_VARIABLES:\n",
    "        if (dataframe[column].dtype == 'int64'):\n",
    "            dataframe[column] = dataframe[column].astype('float64')\n",
    "    return dataframe\n",
    "\n",
    "def _get_impute_parameters_cat(categorical_variables: list) -> dict:\n",
    "    '''\n",
    "    For each column in the categorical features, assign default value for missings.\n",
    "    :param categorical_variables:\n",
    "    :return: impute_parameters\n",
    "    '''\n",
    "\n",
    "    impute_parameters = {}\n",
    "    for column in categorical_variables:\n",
    "        impute_parameters[column] = 'Missing'\n",
    "    return impute_parameters\n",
    "    \n",
    "def _impute_missing_categorical(inputs: dict, target) -> dict:\n",
    "    '''\n",
    "    Given a tf.data.Dataset, impute missing in categorical variables with default 'missing' value\n",
    "    :param inputs:\n",
    "    :param target:\n",
    "    :return: output, target\n",
    "    '''\n",
    "    impute_parameters = _get_impute_parameters_cat(CATEGORICAL_VARIABLES)\n",
    "    # Since we modify just some features, we need to start by setting `outputs` to a copy of `inputs.\n",
    "    output = inputs.copy()\n",
    "    for key, value in impute_parameters.items():\n",
    "        is_blank = tf.math.equal('', inputs[key])\n",
    "        tf_other = tf.constant(value, dtype=np.string_)\n",
    "        output[key] = tf.where(is_blank, tf_other, inputs[key])\n",
    "    return output, target\n",
    "\n",
    "def _get_mean_parameter(dataframe: pd.DataFrame, column: str) -> float:\n",
    "    '''\n",
    "    Given a DataFrame column, calculate mean\n",
    "    :param dataframe: \n",
    "    :param column: \n",
    "    :return: mean\n",
    "    '''\n",
    "    mean = dataframe[column].mean()\n",
    "    return mean\n",
    "\n",
    "def _get_impute_parameters_num(dataframe: pd.DataFrame, numerical_variables: list) -> dict:\n",
    "    '''\n",
    "    Given a DataFrame and its numerical variables, return the associated dictionary of means\n",
    "    :param dataframe: \n",
    "    :param numerical_variables: \n",
    "    :return: impute_parameters\n",
    "    '''\n",
    "\n",
    "    impute_parameters = {}\n",
    "    for column in numerical_variables:\n",
    "        impute_parameters[column] = _get_mean_parameter(dataframe, column)\n",
    "    return impute_parameters\n",
    "\n",
    "def _impute_missing_numerical(inputs: dict, target) -> dict:\n",
    "    '''\n",
    "    Given a tf.data.Dataset, impute missing in numerical variables with training means\n",
    "    :param inputs:\n",
    "    :param target:\n",
    "    :return: output, target\n",
    "    '''\n",
    "    # Get mean parameters for imputing \n",
    "    impute_parameters = _get_impute_parameters_num(data_train, NUMERICAL_VARIABLES) \n",
    "    # Since we modify just some features, we need to start by setting `outputs` to a copy of `inputs.\n",
    "    output = inputs.copy()\n",
    "    for key, value in impute_parameters.items():\n",
    "        # Check if nan (true, false mask)\n",
    "        is_miss = tf.math.is_nan(inputs[key])\n",
    "        # Store mean in a tf.constant\n",
    "        tf_mean = tf.constant(value, dtype=np.float64)\n",
    "        # Impute missing\n",
    "        output[key] = tf.where(is_miss, tf_mean, inputs[key])\n",
    "    return output, target\n",
    "\n",
    "def _get_std_parameter(dataframe:pd.DataFrame, column:str) -> float:\n",
    "    '''\n",
    "    Given a DataFrame column, calculate std\n",
    "    :param dataframe:\n",
    "    :param column:\n",
    "    :return: std\n",
    "    '''\n",
    "    std = dataframe[column].std()\n",
    "    return std\n",
    "\n",
    "def _get_normalization_parameters(numerical_variables: list) -> dict:\n",
    "    '''\n",
    "    For each numerical variable, calculate mean and std based on training dataframe\n",
    "    :param numerical_variables: \n",
    "    :return: normalize_parameters\n",
    "    '''\n",
    "    normalize_parameters = {}\n",
    "    for column in numerical_variables:\n",
    "        normalize_parameters[column] = {}\n",
    "        normalize_parameters[column]['mean'] = _get_mean_parameter(data_train, column)\n",
    "        normalize_parameters[column]['std'] = _get_std_parameter(data_train, column)\n",
    "    return normalize_parameters\n",
    "    \n",
    "def normalizer(column, mean, std):\n",
    "    '''\n",
    "    Given a column, Normalize with calculated mean and std\n",
    "    :param column: \n",
    "    :param mean: \n",
    "    :param std: \n",
    "    :return: \n",
    "    '''\n",
    "    return (column - mean) / std\n",
    "            \n",
    "def check_feature(feature_column):\n",
    "    '''\n",
    "    Given a tf.feature_column and an iter, transform a batch of data\n",
    "    :param feature_column:\n",
    "    :return: None\n",
    "    '''\n",
    "    feature_layer = keras.layers.DenseFeatures(feature_column)\n",
    "    print(feature_layer(example_batch).numpy())\n",
    "    \n",
    "def mlflow_experiment_tracker(algo_name):\n",
    "    \"\"\" \n",
    "    return the run id and experiment id of model\n",
    "    args: \n",
    "      algo_name: the name of algorithm \n",
    "    returns: \n",
    "      model, metrics, run_id, experiment_id\n",
    "    \"\"\"\n",
    "    with mlflow.start_run(run_name=algo_name) as run:\n",
    "        \n",
    "        # Store run_id and experiment_id\n",
    "        run_id=run.info.run_uuid\n",
    "        experiment_id=run.info.experiment_id\n",
    "        \n",
    "        #train the model\n",
    "        model, metrics = train_and_evaluate()\n",
    "        \n",
    "        #Log params\n",
    "        for metric, value in metrics.items():\n",
    "            mlflow.log_metric(metric, value)\n",
    "            \n",
    "    return model, metrics, run_id, experiment_id\n",
    "    \n",
    "def calculate_roc(labels, predictions):\n",
    "    '''\n",
    "    Given labels and predictions columns,\n",
    "    calculare ROC\n",
    "    :param labels:\n",
    "    :param predictions:\n",
    "    :return: fpr, tpr\n",
    "    '''\n",
    "    fpr, tpr, _ = roc_curve(labels, predictions)\n",
    "    return fpr, tpr\n",
    "\n",
    "def calculate_correlation_matrix(labels, predictions, p=0.5):\n",
    "    '''\n",
    "    Given labels and predictions columns,\n",
    "    calculate confusion matrix for a given p\n",
    "    :param labels: \n",
    "    :param predictions: \n",
    "    :param p: \n",
    "    :return: corrmat\n",
    "    '''\n",
    "    corrmat = confusion_matrix(labels, predictions > p)\n",
    "    return corrmat\n",
    "\n",
    "def plot_metrics(name, labels, predictions, p=0.5):\n",
    "    '''\n",
    "    Plot roc and correlation matrix\n",
    "    :param name: \n",
    "    :param labels: \n",
    "    :param predictions: \n",
    "    :param p: \n",
    "    :return: None\n",
    "    '''\n",
    "    metrics = ['roc', 'corrmat']\n",
    "    for n, metric in enumerate(metrics):\n",
    "        plt.subplot(2,2,n+1)\n",
    "        if metric == 'corrmat':\n",
    "            corrmat = calculate_correlation_matrix(labels, predictions, p)\n",
    "            sns.heatmap(corrmat, annot=True, fmt=\"d\")\n",
    "            plt.title(name + ' - ' + 'Confusion matrix with p = {:.2f}'.format(p))\n",
    "            plt.ylabel('Actual label')\n",
    "            plt.xlabel('Predicted label')\n",
    "        if metric == 'roc':\n",
    "            fpr, tpr = calculate_roc(labels=labels, predictions=predictions)\n",
    "            plt.plot(fpr, tpr)\n",
    "            plt.title(name + ' - ' + 'ROC curve')\n",
    "            plt.xlabel('false positive rate')\n",
    "            plt.ylabel('true positive rate')\n",
    "            plt.xlim(0,)\n",
    "            plt.ylim(0,)\n",
    "\n",
    "def print_metrics(corrmat, metrics):\n",
    "    '''\n",
    "    Show evaluation matrix\n",
    "    :param corrmat: \n",
    "    :param metrics: \n",
    "    :return: None\n",
    "    '''\n",
    "    print('Correlation matrix info')\n",
    "    print('True Negatives - No default loans that pay', corrmat[0][0])\n",
    "    print('False Positives - No default loans that dont pay', corrmat[0][1])\n",
    "    print('False Negatives - Default loans that pay', corrmat[1][0])\n",
    "    print('True Positives: - Default loans that dont pay', corrmat[1][1])\n",
    "    print('Total Defauts: ', np.sum(corrmat[1]))\n",
    "    print()\n",
    "    print('-'*20)\n",
    "    print()\n",
    "    print('Evalutation Metrics')\n",
    "    for key, value in metrics.items():\n",
    "        print(key, ':', value)\n",
    "        \n",
    "def setup(folder, modelname):\n",
    "    '''\n",
    "    Given root and model name folder,\n",
    "    remove old version and create a new directory\n",
    "    :param folder: \n",
    "    :param modelname: \n",
    "    :return: model_folder\n",
    "    '''\n",
    "    model_folder = os.path.join(folder, modelname)\n",
    "    #if yes, delete it\n",
    "    if os.path.exists(model_folder):\n",
    "        shutil.rmtree(model_folder)\n",
    "        print(\"Older \", model_folder,\" folder removed!\")\n",
    "    os.makedirs(model_folder)\n",
    "    print(\"Directory \", model_folder,\" created!\")\n",
    "    return model_folder\n",
    "    \n",
    "def copytree(src, dst, symlinks=False, ignore=None):\n",
    "    '''\n",
    "    Given src and dst, \n",
    "    it copies a directory or a files\n",
    "    :param src: \n",
    "    :param dst: \n",
    "    :param symlinks: \n",
    "    :param ignore: \n",
    "    :return: None\n",
    "    '''\n",
    "    for item in os.listdir(src):\n",
    "        s = os.path.join(src, item)\n",
    "        d = os.path.join(dst, item)\n",
    "        if os.path.isdir(s):\n",
    "            shutil.copytree(s, d, symlinks, ignore)\n",
    "        else:\n",
    "            shutil.copy2(s, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 5 ../data/train/train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(TRAIN_DATA_PATH, sep=',')\n",
    "data_test = pd.read_csv(TEST_DATA_PATH, sep=',')                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.head(5)                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_train['BAD'].value_counts()/data_train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: We notice that several variables (numerical and categorical) have missing values. The dataset is unbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Ingestion\n",
    "\n",
    "In this section, I define the input_fn pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Premises\n",
    "\n",
    "Based on documentation, when you import data in Tensorflow you need two elements:\n",
    "\n",
    "**1. input_fn**: specifies how data is converted to a tf.data.Dataset that feeds the input pipeline.\n",
    "\n",
    "**2. feature column**: a construct that indicates a feature's data type.\n",
    "\n",
    "In our case: \n",
    "\n",
    "- We notice that variables have missing. Then we need to impute them. \n",
    "\n",
    "- Also we need to normalize data. \n",
    "\n",
    "And because we want to use Tensorflow framework, we can implement data preprocessing and transformation operations in the TensorFlow model itself. In this way, **it becomes an integral part of the model when the model is exported and deployed for predictions.**\n",
    "\n",
    "TensorFlow transformations can be accomplished in one of the following ways:\n",
    "\n",
    "1. Extending your base feature_columns (using crossed_column, embedding_column, bucketized_column, and so on).\n",
    "\n",
    "2. Implementing all of the instance-level transformation logic in a function that you call in all three input functions: train_input_fn, eval_input_fn, and serving_input_fn.\n",
    "\n",
    "3. If you are creating custom estimators, putting the code in the model_fn function.\n",
    "\n",
    "Then, we have two approaches to inputs:\n",
    "\n",
    "**1. Inside the input_fn**\n",
    "\n",
    "**2. While creating feature_column**\n",
    "\n",
    "Personally I prefer \n",
    "\n",
    "1. Preprocess data in the input_fn \n",
    "\n",
    "2. Do feature engineering while creating feature_column.\n",
    "\n",
    "About **the Data preprocessing strategy of impute missings**, \n",
    "\n",
    "- numerical variables: impute with mean\n",
    "\n",
    "- categorical variables: create 'other' class\n",
    "\n",
    "About **Feature Engineering**, \n",
    "\n",
    "- define normalizer_fn to normalize numerical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define input_fn to create tf.data.Dataset\n",
    "\n",
    "As quick start model we choose Estimator. And It expects their inputs to be formatted as a pair of objects:\n",
    "\n",
    "1. A dictionary in which the keys are feature names and the values are Tensors (or SparseTensors) containing the corresponding feature data\n",
    "2. A Tensor containing one or more labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = ['BAD']\n",
    "CATEGORICAL_VARIABLES = ['REASON', 'JOB']\n",
    "NUMERICAL_VARIABLES = ['LOAN', 'MORTDUE', 'VALUE', 'YOJ', 'DEROG', 'DELINQ', 'CLAGE', 'NINQ', 'CLNO', 'DEBTINC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(dataframe: pd.DataFrame, target: str, num_epochs=2, mode='eval', batch_size=5):\n",
    "    '''\n",
    "    Return input_fn function for TF data ingestion pipeline\n",
    "    :param dataframe: \n",
    "    :param target: \n",
    "    :param num_epochs: \n",
    "    :param mode: \n",
    "    :param batch_size: \n",
    "    :return: input_fn()\n",
    "    '''\n",
    "    def input_fn():\n",
    "        '''\n",
    "        Extract data from pd.DataFrame, Impute and enhance data, Load data in parallel\n",
    "        :return: \n",
    "        '''\n",
    "\n",
    "        # Extract\n",
    "        df = _set_categorical_type(dataframe)\n",
    "        df = _set_categorical_empty(df)\n",
    "        df = _set_numerical_type(df)\n",
    "        predictors = dict(df)\n",
    "        label = predictors.pop(target)\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((predictors, label))\n",
    "\n",
    "        # Transform\n",
    "        dataset = dataset.map(_impute_missing_categorical)\n",
    "        dataset = dataset.map(_impute_missing_numerical)\n",
    "\n",
    "        if mode == 'train':\n",
    "            dataset = dataset.repeat(num_epochs)  # repeat the original dataset 3 times \n",
    "            dataset = dataset.shuffle(buffer_size=1000, seed=8)  # shuffle with a buffer of 1000 element\n",
    "\n",
    "        dataset = dataset.batch(5, drop_remainder=False)  # small batch size to print result\n",
    "\n",
    "        # Load\n",
    "        dataset = dataset.prefetch(1)  # It optimize training parallelizing batch loading over CPU and GPU\n",
    "        \n",
    "        return dataset\n",
    "\n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load small batch to get example and go on with preprocessing\n",
    "train_input_fn = get_dataset(data_train, 'BAD', mode='train', batch_size=5)\n",
    "test_input_fn = get_dataset(data_test, 'BAD', batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a batch example\n",
    "for feature_batch, label_batch in train_input_fn().take(1):\n",
    "    print('Feature keys:', list(feature_batch.keys()))\n",
    "    print('A batch of REASON:', feature_batch['REASON'].numpy())\n",
    "    print('A batch of Labels:', label_batch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Engineering (Preprocessing)\n",
    "\n",
    "In this section, I implemented feature engineering as Tensorflow Framework requires."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define features and configures feature_columns\n",
    "\n",
    "In order to import our training data into TensorFlow, we need to specify what type of data each feature contains. \n",
    "\n",
    "In our case, we have:\n",
    "\n",
    "1. **Categorical Data**: 'REASON', 'JOB'\n",
    "\n",
    "2. **Numerical Data**: 'LOAN', 'MORTDUE', 'VALUE', 'YOJ', 'DEROG', 'DELINQ', 'CLAGE', 'NINQ', 'CLNO', 'DEBTINC'\n",
    "\n",
    "In TensorFlow, we indicate a feature's data type using a construct called a **feature column**.\n",
    "\n",
    "Feature columns store only a description of the feature data; they do not contain the feature data itself.\n",
    "\n",
    "**Notice**: So far we mentioned that we can extend their functionalies in order to apply prepocessing transformations. That's what we are going to do: \n",
    "\n",
    "1. Normalize Numerical variables with normalizer_fn\n",
    "\n",
    "2. OneHot encoding with tf.feature_column.categorical_column_with_vocabulary_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(num_features: list, cat_features: list, labels_dict: dict) -> list:\n",
    "    '''\n",
    "    Return a list of tf feature columns\n",
    "    :param num_features: \n",
    "    :param cat_features: \n",
    "    :param labels_dict: \n",
    "    :return: feature_columns\n",
    "    '''\n",
    "    # Create an empty list for feature\n",
    "    feature_columns = []\n",
    "\n",
    "    # Get numerical features\n",
    "    normalize_parameters = _get_normalization_parameters(num_features)\n",
    "    for col_name in num_features:\n",
    "        mean = normalize_parameters[col_name]['mean']\n",
    "        std = normalize_parameters[col_name]['std']\n",
    "        normalizer_fn = functools.partial(normalizer, mean=mean, std=std)\n",
    "        num_feature = tf.feature_column.numeric_column(col_name, dtype=tf.float32, normalizer_fn=normalizer_fn)\n",
    "        feature_columns.append(num_feature)\n",
    "\n",
    "    # Get categorical features\n",
    "    for col_name in cat_features:\n",
    "        cat_feature = tf.feature_column.categorical_column_with_vocabulary_list(col_name, labels_dict[col_name])\n",
    "        indicator_column = tf.feature_column.indicator_column(cat_feature)\n",
    "        feature_columns.append(indicator_column)\n",
    "\n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tf.data.dataset and an iterator to check transformations\n",
    "train_dataset = train_input_fn()            \n",
    "example_batch = next(iter(train_dataset))[0]\n",
    "\n",
    "labels_dict= {'REASON': ['DebtCon', 'HomeImp', 'Missing'],\n",
    "              'JOB' : ['Other', 'Sales', 'ProfExe', 'Office', 'Mgr', 'Self', 'Missing']}\n",
    "feature_columns = get_features(NUMERICAL_VARIABLES, CATEGORICAL_VARIABLES, labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns[::3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_feature(feature_columns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_feature(feature_columns[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Experiments (Training)\n",
    "\n",
    "In this section, I train different models to get the Champion.\n",
    "\n",
    "In particular, \n",
    "\n",
    "    a. Model A: pre-made Estimator - LinearClassifier\n",
    "    b. Model B: pre-made Estimator - BoostedTreesClassifier\n",
    "    \n",
    "In this way, I can compare both model based on TF API and model based on my business knowledge.\n",
    "    \n",
    "**Notice**: Dataset is heavly inbalanced. Because it is a plain vanilla example I'm not going to deal with that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model A: pre-made Estimator - LinearClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the pre-made Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_estimator (feature_columns, learning_rate=0.1):\n",
    "    '''\n",
    "    Given feature columns, \n",
    "    build a LinearClassifier Estimator\n",
    "    :param feature_columns:\n",
    "    :param learning_rate:\n",
    "    :return:\n",
    "    '''\n",
    "    feature_layer = tf.keras.layers.DenseFeatures(feature_columns, dtype='float32')\n",
    "\n",
    "    runconfig = tf.estimator.RunConfig(tf_random_seed=8)\n",
    "\n",
    "    linear_classifier_base = tf.estimator.LinearClassifier(\n",
    "        model_dir=LOGS_DIR,\n",
    "        feature_columns=feature_columns,\n",
    "        n_classes=2,\n",
    "        optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "    )\n",
    "\n",
    "    return linear_classifier_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = build_estimator(feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train pre-made Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate():\n",
    "    # Get dataset\n",
    "    train_input_fn = get_dataset(data_train, 'BAD', batch_size=500, mode='train')\n",
    "    test_input_fn = get_dataset(data_test, 'BAD', batch_size=500)\n",
    "    # Get Features\n",
    "    feature_columns = get_features(NUMERICAL_VARIABLES, CATEGORICAL_VARIABLES, labels_dict)\n",
    "    #Clean all\n",
    "    shutil.rmtree(LOGS_DIR, ignore_errors = True) \n",
    "    # Get estimator\n",
    "    estimator = build_estimator(feature_columns, learning_rate=0.1)\n",
    "    # Train the estimator\n",
    "    estimator_train = estimator.train(input_fn=train_input_fn)\n",
    "    # Evaluate \n",
    "    metrics = estimator_train.evaluate(input_fn=test_input_fn)\n",
    "    return estimator_train, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, metrics, _, _= mlflow_experiment_tracker('LinearClassifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate pre-made Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dictionary = list(model.predict(test_input_fn))\n",
    "predictions = pd.Series([pred['class_ids'] for pred in predictions_dictionary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = data_test['BAD']\n",
    "plot_metrics(\"Test sample\", test_labels, predictions, p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(calculate_correlation_matrix(test_labels, predictions), metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model B: pre-made Estimator - BoostedTreesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the pre-made Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_LAYER = 29\n",
    "def build_estimator(feature_columns, learning_rate=0.1):\n",
    "    '''\n",
    "    Given feature columns, \n",
    "    build a BoostedTreesClassifier Estimator\n",
    "    :param feature_columns:\n",
    "    :param learning_rate:\n",
    "    :return:\n",
    "    '''\n",
    "    feature_layer = tf.keras.layers.DenseFeatures(feature_columns, dtype='float32')\n",
    "    \n",
    "    runconfig = tf.estimator.RunConfig(tf_random_seed=8)\n",
    "    \n",
    "    boosted_trees_classifier = tf.estimator.BoostedTreesClassifier(\n",
    "    model_dir=LOGS_DIR,\n",
    "    feature_columns=feature_columns,\n",
    "    n_classes=2,\n",
    "    n_batches_per_layer=BATCH_LAYER,\n",
    "    learning_rate=learning_rate\n",
    "    )\n",
    "    \n",
    "    return boosted_trees_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = build_estimator(feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train pre-made Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate():\n",
    "    # Get dataset\n",
    "    train_input_fn = get_dataset(data_train, 'BAD', batch_size=500, mode='train')\n",
    "    test_input_fn = get_dataset(data_test, 'BAD', batch_size=500)\n",
    "    # Get Features\n",
    "    feature_columns = get_features(NUMERICAL_VARIABLES, CATEGORICAL_VARIABLES, labels_dict)\n",
    "    #Clean all\n",
    "    shutil.rmtree(LOGS_DIR, ignore_errors = True) \n",
    "    # Get estimator\n",
    "    estimator = build_estimator(feature_columns)\n",
    "    # Train the estimator\n",
    "    estimator_train = estimator.train(input_fn=train_input_fn)\n",
    "    # Evaluate \n",
    "    metrics = estimator_train.evaluate(input_fn=test_input_fn)\n",
    "    return estimator_train, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, metrics, _, _= mlflow_experiment_tracker('BoostedTreesClassifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate pre-made Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dictionary = list(model.predict(test_input_fn))\n",
    "predictions = pd.Series([pred['class_ids'] for pred in predictions_dictionary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = data_test['BAD']\n",
    "plot_metrics(\"Test sample\", test_labels, predictions, p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(calculate_correlation_matrix(test_labels, predictions), metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which is the best model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After compare them in Mlflow, we assume that **the best model is boosted_trees_classifier**\n",
    "\n",
    "**Comment**: Models are really bad. Data are too much unbalanced. Need to change the strategy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup(BASE_DIR_PATH, '../models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 1\n",
    "DATE = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "ID = \"_\".join([str(DATE), str(VERSION)])\n",
    "EXPORT_PATH = os.path.join(MODELS_DIR, ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(tf.feature_column.make_parse_example_spec(feature_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.export_saved_model(EXPORT_PATH, serving_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export DIR=/home/jovyan/work/notebooks/../models/20201019055831_1/1603087114\n",
    "saved_model_cli show \\\n",
    " --tag_set serve \\\n",
    " --signature_def serving_default \\\n",
    " --dir $DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test it with Docker via Restful API\n",
    "\n",
    "I could do that using Python Docker client. \n",
    "\n",
    "Check my project here: https://github.com/IvanNardini/modelops-aws-web-endpoint-hosted/blob/master/notebooks/02_engineering.ipynb\n",
    "\n",
    "In that case, go on terminal and run the following commands.\n",
    "\n",
    "In my case...\n",
    "\n",
    "```bash\n",
    "docker pull tensorflow/serving\n",
    "EXPORT_BASE_DIR=/home/ubuntu/modelops-sas-tensorflow-workflow-manager-openshift/models/20200927165533_1/\n",
    "docker run -t --rm --name tf -p 8501:8501 -v \"$EXPORT_BASE_DIR:/models/model\" tensorflow/serving\n",
    "SERVER=$(hostname -I | cut -d ' ' -f1)\n",
    "curl -d '{\"examples\":[{\"LOAN\": 34400.0, \"MORTDUE\": 97971.0, \"VALUE\": 145124.0, \"YOJ\": 13.0, \"DEROG\": 0.0, \"DELINQ\": 0.0, \"CLAGE\": 67.832, \"NINQ\": 1.0, \"CLNO\": 36.0, \"DEBTINC\": 40.402, \"REASON\": \"DebtCon\", \"JOB\": \"Other\"}]}' -X POST http://$SERVER:8501/v1/models/model:classify\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Wrap Up and Project Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Champion model is **boosted_trees_classifier**\n",
    "\n",
    "But because data are too much unbalanced. Need to change the strategy. \n",
    "\n",
    "Futher investigations are required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Champion Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAMPION_PATH = '/home/jovyan/work/notebooks/../models/20201019055831_1'\n",
    "DELIVERS_DIR = os.path.join(BASE_DIR_PATH, '../deliverables')\n",
    "CHAMPION_DELIVER_PATH = '/home/jovyan/work/notebooks/../deliverables/champion'\n",
    "\n",
    "setup(DELIVERS_DIR, 'champion')\n",
    "copytree(CHAMPION_PATH, CHAMPION_DELIVER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
