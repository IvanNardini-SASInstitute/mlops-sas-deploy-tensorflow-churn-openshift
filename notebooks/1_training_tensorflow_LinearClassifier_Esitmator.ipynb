{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "This notebook provides an example of how to train Tensorflow classifiers using the HMEQ dataset\n",
    "\n",
    "The goal is to predict whether a customer is a BAD (default) borrower, which in this dataset is a binary classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumption\n",
    "\n",
    "We are working in big data context. \n",
    "\n",
    "Then, I'm going to work with HMEQ dataset as it is so large that it would not fit in RAM. \n",
    "\n",
    "Then we use the Tensorflow framework to deal with that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#General\n",
    "import os\n",
    "import functools\n",
    "import pprint\n",
    "import tempfile\n",
    "\n",
    "#Analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR_PATH = os.getcwd()\n",
    "DATA_DIR_PATH = os.path.join(BASE_DIR_PATH, '../data')\n",
    "\n",
    "# Data directories paths\n",
    "TRAIN_DIR_PATH = os.path.join(DATA_DIR_PATH, 'train')\n",
    "TEST_DIR_PATH = os.path.join(DATA_DIR_PATH, 'test')\n",
    "VAL_DIR_PATH = os.path.join(DATA_DIR_PATH, 'val')\n",
    "\n",
    "# Data file paths\n",
    "TRAIN_DATA_PATH = os.path.join(TRAIN_DIR_PATH, 'train.csv')\n",
    "TEST_DATA_PATH = os.path.join(TEST_DIR_PATH, 'test.csv')\n",
    "VAL_DATA_PATH = os.path.join(VAL_DIR_PATH, 'val.csv')\n",
    "\n",
    "# Model directories\n",
    "LOG_DIR = os.path.join(BASE_DIR_PATH, '../logs')\n",
    "MODEL_DIR = os.path.join(BASE_DIR_PATH, '../models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing data\n",
    "def _set_categorical_type(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''Set the categorical type as string if needed'''\n",
    "    for column in CATEGORICAL_VARIABLES:\n",
    "        if (dataframe[column].dtype == 'O'):\n",
    "            dataframe[column] = dataframe[column].astype('string')\n",
    "    return dataframe\n",
    "\n",
    "def _set_categorical_empty(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''Change object type for categorical variable to avoid TF issue '''\n",
    "    for column in CATEGORICAL_VARIABLES:\n",
    "        if any(dataframe[column].isna()):\n",
    "            dataframe[column] = dataframe[column].fillna('')\n",
    "    return dataframe\n",
    "\n",
    "def _set_numerical_type(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''Set the numerical type as float64 if needed'''\n",
    "    for column in NUMERICAL_VARIABLES:\n",
    "        if (dataframe[column].dtype == 'int64'):\n",
    "            dataframe[column] = dataframe[column].astype('float64')\n",
    "    return dataframe\n",
    "\n",
    "def _get_impute_parameters_cat(categorical_variables: list) -> dict:\n",
    "    '''For each column in the numerical features, calculate mean.'''\n",
    "    impute_parameters = {}\n",
    "    for column in categorical_variables:\n",
    "        impute_parameters[column] = 'Missing'\n",
    "    return impute_parameters\n",
    "    \n",
    "def _impute_missing_categorical(inputs: dict, target) -> dict:\n",
    "    impute_parameters = _get_impute_parameters_cat(CATEGORICAL_VARIABLES)\n",
    "    # Since we modify just some features, \n",
    "    # we need to start by setting `outputs` to a copy of `inputs.\n",
    "    output = inputs.copy()\n",
    "    for key, value in impute_parameters.items():\n",
    "        is_blank = tf.math.equal('', inputs[key])\n",
    "        tf_other = tf.constant(value, dtype=np.string_)\n",
    "        output[key] = tf.where(is_blank, tf_other, inputs[key])\n",
    "    return output, target\n",
    "\n",
    "def _get_mean_parameter(dataframe: pd.DataFrame, column: str) -> float:\n",
    "    ''' Given a column, calculate mean'''\n",
    "    mean = dataframe[column].mean()\n",
    "    return mean\n",
    "\n",
    "def _get_impute_parameters_num(dataframe: pd.DataFrame, numerical_variables: list) -> dict:\n",
    "    '''For each column in the numerical features, calculate mean.'''\n",
    "    impute_parameters = {}\n",
    "    for column in numerical_variables:\n",
    "        impute_parameters[column] = _get_mean_parameter(dataframe, column)\n",
    "    return impute_parameters\n",
    "\n",
    "def _impute_missing_numerical(inputs: dict, target) -> dict:\n",
    "    '''Impute missing based on training mean'''\n",
    "    impute_parameters = _get_impute_parameters_num(data_train, NUMERICAL_VARIABLES) ## Here we have data_train\n",
    "    # Since we modify just some features, \n",
    "    # we need to start by setting `outputs` to a copy of `inputs.\n",
    "    output = inputs.copy()\n",
    "    for key, value in impute_parameters.items():\n",
    "        is_miss = tf.math.is_nan(inputs[key])\n",
    "        tf_mean = tf.constant(value, dtype=np.float64)\n",
    "        output[key] = tf.where(is_miss, tf_mean, inputs[key])\n",
    "    return output, target\n",
    "\n",
    "def _get_std_parameter(dataframe:pd.DataFrame, column:str) -> float:\n",
    "    '''Given a column, calculate sd'''\n",
    "    std = dataframe[column].std()\n",
    "    return std\n",
    "\n",
    "def _get_normalization_parameters(numerical_variables: list) -> dict:\n",
    "    '''For each column in the numerical features, calculate mean and std.'''\n",
    "    normalize_parameters = {}\n",
    "    for column in numerical_variables:\n",
    "        normalize_parameters[column] = {}\n",
    "        normalize_parameters[column]['mean'] = _get_mean_parameter(data_train, column)\n",
    "        normalize_parameters[column]['std'] = _get_std_parameter(data_train, column)\n",
    "    return normalize_parameters\n",
    "    \n",
    "def normalizer_fn(column:str):\n",
    "    mean = normalize_parameters[column]['mean']\n",
    "    std = normalize_parameters[column]['std']\n",
    "    return (column - mean) / std\n",
    "            \n",
    "# A utility method to create a feature column\n",
    "# and to transform a batch of data\n",
    "def check_feature(feature_column):\n",
    "    feature_layer = layers.DenseFeatures(feature_column)\n",
    "    print(feature_layer(example_batch).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAD,LOAN,MORTDUE,VALUE,REASON,JOB,YOJ,DEROG,DELINQ,CLAGE,NINQ,CLNO,DEBTINC\r\n",
      "0,34400,97971.0,145124.0,DebtCon,Other,13.0,0.0,0.0,67.8320416646805,1.0,36.0,40.4027058419691\r\n",
      "0,13600,89937.0,110986.0,DebtCon,Sales,14.0,,2.0,146.718742448452,1.0,17.0,33.7471158335903\r\n",
      "1,10800,75000.0,87400.0,HomeImp,Other,7.0,1.0,0.0,101.46666666666701,2.0,19.0,\r\n",
      "0,14900,87167.0,114219.0,DebtCon,ProfExe,8.0,0.0,0.0,194.113173533089,2.0,36.0,41.3297639035293\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 5 ../data/train/train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BAD</th>\n",
       "      <th>LOAN</th>\n",
       "      <th>MORTDUE</th>\n",
       "      <th>VALUE</th>\n",
       "      <th>REASON</th>\n",
       "      <th>JOB</th>\n",
       "      <th>YOJ</th>\n",
       "      <th>DEROG</th>\n",
       "      <th>DELINQ</th>\n",
       "      <th>CLAGE</th>\n",
       "      <th>NINQ</th>\n",
       "      <th>CLNO</th>\n",
       "      <th>DEBTINC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>34400</td>\n",
       "      <td>97971.0</td>\n",
       "      <td>145124.0</td>\n",
       "      <td>DebtCon</td>\n",
       "      <td>Other</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67.832042</td>\n",
       "      <td>1.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>40.402706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>13600</td>\n",
       "      <td>89937.0</td>\n",
       "      <td>110986.0</td>\n",
       "      <td>DebtCon</td>\n",
       "      <td>Sales</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>146.718742</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>33.747116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>10800</td>\n",
       "      <td>75000.0</td>\n",
       "      <td>87400.0</td>\n",
       "      <td>HomeImp</td>\n",
       "      <td>Other</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101.466667</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>14900</td>\n",
       "      <td>87167.0</td>\n",
       "      <td>114219.0</td>\n",
       "      <td>DebtCon</td>\n",
       "      <td>ProfExe</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>194.113174</td>\n",
       "      <td>2.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>41.329764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>7200</td>\n",
       "      <td>98691.0</td>\n",
       "      <td>115750.0</td>\n",
       "      <td>HomeImp</td>\n",
       "      <td>Office</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>118.000142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>37.720359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   BAD   LOAN  MORTDUE     VALUE   REASON      JOB   YOJ  DEROG  DELINQ  \\\n",
       "0    0  34400  97971.0  145124.0  DebtCon    Other  13.0    0.0     0.0   \n",
       "1    0  13600  89937.0  110986.0  DebtCon    Sales  14.0    NaN     2.0   \n",
       "2    1  10800  75000.0   87400.0  HomeImp    Other   7.0    1.0     0.0   \n",
       "3    0  14900  87167.0  114219.0  DebtCon  ProfExe   8.0    0.0     0.0   \n",
       "4    0   7200  98691.0  115750.0  HomeImp   Office  22.0    0.0     0.0   \n",
       "\n",
       "        CLAGE  NINQ  CLNO    DEBTINC  \n",
       "0   67.832042   1.0  36.0  40.402706  \n",
       "1  146.718742   1.0  17.0  33.747116  \n",
       "2  101.466667   2.0  19.0        NaN  \n",
       "3  194.113174   2.0  36.0  41.329764  \n",
       "4  118.000142   0.0  11.0  37.720359  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_csv(TRAIN_DATA_PATH, sep=',')\n",
    "data_test = pd.read_csv(TEST_DATA_PATH, sep=',')\n",
    "data_val = pd.read_csv(VAL_DATA_PATH, sep=',')\n",
    "\n",
    "data_train.head(5)                                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4827 entries, 0 to 4826\n",
      "Data columns (total 13 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   BAD      4827 non-null   int64  \n",
      " 1   LOAN     4827 non-null   int64  \n",
      " 2   MORTDUE  4405 non-null   float64\n",
      " 3   VALUE    4738 non-null   float64\n",
      " 4   REASON   4618 non-null   object \n",
      " 5   JOB      4601 non-null   object \n",
      " 6   YOJ      4419 non-null   float64\n",
      " 7   DEROG    4262 non-null   float64\n",
      " 8   DELINQ   4362 non-null   float64\n",
      " 9   CLAGE    4578 non-null   float64\n",
      " 10  NINQ     4421 non-null   float64\n",
      " 11  CLNO     4651 non-null   float64\n",
      " 12  DEBTINC  3804 non-null   float64\n",
      "dtypes: float64(9), int64(2), object(2)\n",
      "memory usage: 490.4+ KB\n"
     ]
    }
   ],
   "source": [
    "data_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BAD</th>\n",
       "      <td>4827.0</td>\n",
       "      <td>0.198674</td>\n",
       "      <td>0.399043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOAN</th>\n",
       "      <td>4827.0</td>\n",
       "      <td>18617.112078</td>\n",
       "      <td>11231.974061</td>\n",
       "      <td>1100.000000</td>\n",
       "      <td>11100.000000</td>\n",
       "      <td>16300.000000</td>\n",
       "      <td>23300.000000</td>\n",
       "      <td>89900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MORTDUE</th>\n",
       "      <td>4405.0</td>\n",
       "      <td>73796.339364</td>\n",
       "      <td>43741.460123</td>\n",
       "      <td>2063.000000</td>\n",
       "      <td>46884.000000</td>\n",
       "      <td>65206.000000</td>\n",
       "      <td>91491.000000</td>\n",
       "      <td>399412.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VALUE</th>\n",
       "      <td>4738.0</td>\n",
       "      <td>101633.021895</td>\n",
       "      <td>56564.609914</td>\n",
       "      <td>8000.000000</td>\n",
       "      <td>66260.250000</td>\n",
       "      <td>89407.500000</td>\n",
       "      <td>119732.500000</td>\n",
       "      <td>855909.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YOJ</th>\n",
       "      <td>4419.0</td>\n",
       "      <td>8.957377</td>\n",
       "      <td>7.604500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>41.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEROG</th>\n",
       "      <td>4262.0</td>\n",
       "      <td>0.244486</td>\n",
       "      <td>0.823733</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DELINQ</th>\n",
       "      <td>4362.0</td>\n",
       "      <td>0.446813</td>\n",
       "      <td>1.138853</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLAGE</th>\n",
       "      <td>4578.0</td>\n",
       "      <td>179.902913</td>\n",
       "      <td>86.744368</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>114.858318</td>\n",
       "      <td>173.497696</td>\n",
       "      <td>231.876088</td>\n",
       "      <td>1168.233561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NINQ</th>\n",
       "      <td>4421.0</td>\n",
       "      <td>1.199276</td>\n",
       "      <td>1.745287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLNO</th>\n",
       "      <td>4651.0</td>\n",
       "      <td>21.322081</td>\n",
       "      <td>10.111162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>71.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEBTINC</th>\n",
       "      <td>3804.0</td>\n",
       "      <td>33.768804</td>\n",
       "      <td>8.806656</td>\n",
       "      <td>0.524499</td>\n",
       "      <td>29.149239</td>\n",
       "      <td>34.818353</td>\n",
       "      <td>38.972750</td>\n",
       "      <td>203.312149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          count           mean           std          min           25%  \\\n",
       "BAD      4827.0       0.198674      0.399043     0.000000      0.000000   \n",
       "LOAN     4827.0   18617.112078  11231.974061  1100.000000  11100.000000   \n",
       "MORTDUE  4405.0   73796.339364  43741.460123  2063.000000  46884.000000   \n",
       "VALUE    4738.0  101633.021895  56564.609914  8000.000000  66260.250000   \n",
       "YOJ      4419.0       8.957377      7.604500     0.000000      3.000000   \n",
       "DEROG    4262.0       0.244486      0.823733     0.000000      0.000000   \n",
       "DELINQ   4362.0       0.446813      1.138853     0.000000      0.000000   \n",
       "CLAGE    4578.0     179.902913     86.744368     0.000000    114.858318   \n",
       "NINQ     4421.0       1.199276      1.745287     0.000000      0.000000   \n",
       "CLNO     4651.0      21.322081     10.111162     0.000000     15.000000   \n",
       "DEBTINC  3804.0      33.768804      8.806656     0.524499     29.149239   \n",
       "\n",
       "                  50%            75%            max  \n",
       "BAD          0.000000       0.000000       1.000000  \n",
       "LOAN     16300.000000   23300.000000   89900.000000  \n",
       "MORTDUE  65206.000000   91491.000000  399412.000000  \n",
       "VALUE    89407.500000  119732.500000  855909.000000  \n",
       "YOJ          7.000000      13.000000      41.000000  \n",
       "DEROG        0.000000       0.000000      10.000000  \n",
       "DELINQ       0.000000       0.000000      15.000000  \n",
       "CLAGE      173.497696     231.876088    1168.233561  \n",
       "NINQ         1.000000       2.000000      17.000000  \n",
       "CLNO        20.000000      26.000000      71.000000  \n",
       "DEBTINC     34.818353      38.972750     203.312149  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.801326\n",
       "1    0.198674\n",
       "Name: BAD, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['BAD'].value_counts()/data_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: We notice that several variables (numerical and categorical) have missing values. The dataset is unbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data in Tensorflow\n",
    "\n",
    "Based on what I understood when you import data in Tensorflow you need two elements:\n",
    "\n",
    "**1. input_fn**: specifies how data is converted to a tf.data.Dataset that feeds the input pipeline.\n",
    "\n",
    "**2. feature column**: a construct that indicates a feature's data type.\n",
    "\n",
    "In our case, we notice that variables have missing. Then we need to impute them. Also we need to normalize data. \n",
    "\n",
    "And because we want to use Tensorflow framework, we can implement data preprocessing and transformation operations in the TensorFlow model itself. In this way, **it becomes an integral part of the model when the model is exported and deployed for predictions.**\n",
    "\n",
    "TensorFlow transformations can be accomplished in one of the following ways:\n",
    "\n",
    "1. Extending your base feature_columns (using crossed_column, embedding_column, bucketized_column, and so on).\n",
    "\n",
    "2. Implementing all of the instance-level transformation logic in a function that you call in all three input functions: train_input_fn, eval_input_fn, and serving_input_fn.\n",
    "\n",
    "3. If you are creating custom estimators, putting the code in the model_fn function.\n",
    "\n",
    "Then, we have two approaches to inputs:\n",
    "\n",
    "**1. Inside the input_fn**\n",
    "\n",
    "**2. While creating feature_column**\n",
    "\n",
    "Personally I prefer \n",
    "\n",
    "1. Preprocess data in the input_fn \n",
    "\n",
    "2. Do feature engineering while creating feature_column.\n",
    "\n",
    "About **the Data preprocessing strategy of impute missings**, \n",
    "\n",
    "- numerical variables: impute with mean\n",
    "\n",
    "- categorical variables: create 'other' class\n",
    "\n",
    "About **Feature Engineering**, \n",
    "\n",
    "- define normalizer_fn to normalize numerical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define input_fn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = ['BAD']\n",
    "CATEGORICAL_VARIABLES = ['REASON', 'JOB']\n",
    "NUMERICAL_VARIABLES = ['LOAN', 'MORTDUE', 'VALUE', 'YOJ', 'DEROG', 'DELINQ', 'CLAGE', 'NINQ', 'CLNO', 'DEBTINC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the input function to load data into dataset\n",
    "\n",
    "def get_dataset(dataframe:pd.DataFrame, target:str, num_epochs=2, mode='eval', batch_size=5):\n",
    "    \n",
    "    def input_fn():\n",
    "        '''input_fn to read the data and impute missings'''\n",
    "        \n",
    "        # Extract\n",
    "        df = _set_categorical_type(dataframe)\n",
    "        df = _set_categorical_empty(df)\n",
    "        df = _set_numerical_type(df)\n",
    "        predictors = dict(df)\n",
    "        label = predictors.pop(target)\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((predictors, label))\n",
    "        \n",
    "        #Transform\n",
    "        dataset = dataset.map(_impute_missing_categorical)\n",
    "        dataset = dataset.map(_impute_missing_numerical)\n",
    "        \n",
    "        if mode == 'train':\n",
    "            dataset = dataset.repeat(num_epochs) # repeat the original dataset 3 times \n",
    "            dataset = dataset.shuffle(buffer_size=1000, seed=8) # shuffle with a buffer of 1000 element\n",
    "            \n",
    "        dataset = dataset.batch(5, drop_remainder=True) # small batch size to print result\n",
    "        \n",
    "        #Load\n",
    "        dataset = dataset.prefetch(1) #just to use it. It optimize training parallelizing batch loading over CPU and GPU\n",
    "            \n",
    "        #Load: to check\n",
    "        return dataset\n",
    "    \n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = get_dataset(data_train, 'BAD', mode='train')\n",
    "test_input_fn = get_dataset(data_test, 'BAD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature keys: ['LOAN', 'MORTDUE', 'VALUE', 'REASON', 'JOB', 'YOJ', 'DEROG', 'DELINQ', 'CLAGE', 'NINQ', 'CLNO', 'DEBTINC']\n",
      "A batch of REASON: [b'DebtCon' b'DebtCon' b'DebtCon' b'DebtCon' b'DebtCon']\n",
      "A batch of Labels: [0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "for feature_batch, label_batch in train_input_fn().take(1):\n",
    "    print('Feature keys:', list(feature_batch.keys()))\n",
    "    print('A batch of REASON:', feature_batch['REASON'].numpy())\n",
    "    print('A batch of Labels:', label_batch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define features and configures feature_columns\n",
    "\n",
    "In order to import our training data into TensorFlow, we need to specify what type of data each feature contains. \n",
    "\n",
    "In our case, we have:\n",
    "\n",
    "1. **Categorical Data**: 'REASON', 'JOB'\n",
    "\n",
    "2. **Numerical Data**: 'LOAN', 'MORTDUE', 'VALUE', 'YOJ', 'DEROG', 'DELINQ', 'CLAGE', 'NINQ', 'CLNO', 'DEBTINC'\n",
    "\n",
    "In TensorFlow, we indicate a feature's data type using a construct called a **feature column**. \n",
    "\n",
    "Feature columns store only a description of the feature data; they do not contain the feature data itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_input_fn()            \n",
    "# We will use this batch to demonstrate several types of feature columns\n",
    "example_batch = next(iter(train_dataset))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical variables\n",
    "\n",
    "normalize_parameters = _get_normalization_parameters(NUMERICAL_VARIABLES)\n",
    "\n",
    "for col_name in NUMERICAL_VARIABLES:\n",
    "    num_feature = tf.feature_column.numeric_column(col_name, dtype=tf.float64, normalizer_fn=normalizer_fn)\n",
    "    feature_columns.append(num_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical variables\n",
    "\n",
    "labels_dict= {'REASON': ['DebtCon', 'HomeImp', 'Missing'],\n",
    "              'JOB' : ['Other', 'Sales', 'ProfExe', 'Office', 'Mgr', 'Self', 'Missing']}\n",
    "\n",
    "for col_name in CATEGORICAL_VARIABLES:\n",
    "    cat_feature = tf.feature_column.categorical_column_with_vocabulary_list(col_name, labels_dict[col_name])\n",
    "    indicator_column = tf.feature_column.indicator_column(cat_feature)\n",
    "    feature_columns.append(indicator_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dense_features is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "[[1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "check_feature(feature_columns[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NumericColumn(key='LOAN', shape=(1,), default_value=None, dtype=tf.float64, normalizer_fn=<function normalizer_fn at 0x7f014da001f0>),\n",
       " NumericColumn(key='VALUE', shape=(1,), default_value=None, dtype=tf.float64, normalizer_fn=<function normalizer_fn at 0x7f014da001f0>),\n",
       " NumericColumn(key='DEROG', shape=(1,), default_value=None, dtype=tf.float64, normalizer_fn=<function normalizer_fn at 0x7f014da001f0>),\n",
       " NumericColumn(key='CLAGE', shape=(1,), default_value=None, dtype=tf.float64, normalizer_fn=<function normalizer_fn at 0x7f014da001f0>),\n",
       " NumericColumn(key='CLNO', shape=(1,), default_value=None, dtype=tf.float64, normalizer_fn=<function normalizer_fn at 0x7f014da001f0>),\n",
       " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='REASON', vocabulary_list=('DebtCon', 'HomeImp', 'Missing'), dtype=tf.string, default_value=-1, num_oov_buckets=0))]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show some examples\n",
    "feature_columns[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a get_features function\n",
    "def get_features(num_features: list, cat_features: list, labels_dict:dict) -> list:\n",
    "    \n",
    "    # Create an empty list for feature\n",
    "    feature_columns = []\n",
    "    \n",
    "    #Get numerical features\n",
    "    normalize_parameters = _get_normalization_parameters(NUMERICAL_VARIABLES)\n",
    "    for col_name in NUMERICAL_VARIABLES:\n",
    "        num_feature = tf.feature_column.numeric_column(col_name, dtype=tf.float64, normalizer_fn=normalizer_fn)\n",
    "        feature_columns.append(num_feature)\n",
    "    \n",
    "    #Get categorical features\n",
    "    for col_name in cat_features:\n",
    "        cat_feature = tf.feature_column.categorical_column_with_vocabulary_list(col_name, labels_dict[col_name])\n",
    "        indicator_column = tf.feature_column.indicator_column(cat_feature)\n",
    "        feature_columns.append(indicator_column)\n",
    "        \n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = get_features(NUMERICAL_VARIABLES, CATEGORICAL_VARIABLES, labels_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Feature Layer\n",
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/home/jovyan/work/notebooks/../logs', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "# Use Base Estimator classifier\n",
    "linear_classifier_base = tf.estimator.LinearClassifier(\n",
    "    model_dir=LOG_DIR, \n",
    "    feature_columns=feature_columns,\n",
    "    n_classes=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_estimator(feature_columns, learning_rate=0.1):\n",
    "    \"\"\"\n",
    "     Build an estimator.\n",
    "     \n",
    "    \"\"\"\n",
    "    feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
    "    \n",
    "    runconfig = tf.estimator.RunConfig(tf_random_seed=8)\n",
    "    \n",
    "    linear_classifier_base = tf.estimator.LinearClassifier(\n",
    "    model_dir=LOG_DIR, \n",
    "    feature_columns=feature_columns,\n",
    "    n_classes=2,\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "    )\n",
    "    \n",
    "    return linear_classifier_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/home/jovyan/work/notebooks/../logs', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "estimator = build_estimator(feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.8/site-packages/tensorflow/python/training/training_util.py:235: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer linear/linear_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/linear.py:1471: Layer.add_variable (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n"
     ]
    },
    {
     "ename": "StagingError",
     "evalue": "in user code:\n\n    /opt/conda/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/linear.py:1671 call  *\n        return self.layer(features)\n    /opt/conda/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/linear.py:1499 call  *\n        weighted_sum = fc_v2._create_weighted_sum(  # pylint: disable=protected-access\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/feature_column/feature_column_v2.py:2140 _create_weighted_sum  **\n        return _create_dense_column_weighted_sum(\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/feature_column/feature_column_v2.py:2150 _create_dense_column_weighted_sum\n        tensor = column.get_dense_tensor(transformation_cache, state_manager)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/feature_column/feature_column_v2.py:2592 get_dense_tensor\n        return transformation_cache.get(self, state_manager)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/feature_column/feature_column_v2.py:2355 get\n        transformed = column.transform_feature(self, state_manager)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/feature_column/feature_column_v2.py:2565 transform_feature\n        return self._transform_input_tensor(input_tensor)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/feature_column/feature_column_v2.py:2539 _transform_input_tensor\n        input_tensor = self.normalizer_fn(input_tensor)\n    <ipython-input-14-9718c5439d32>:80 normalizer_fn\n        mean = normalize_parameters[column]['mean']\n\n    KeyError: <tf.Tensor 'linear/linear_model/linear/linear_model/linear/linear_model/CLAGE/ExpandDims:0' shape=(5, 1) dtype=float32>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStagingError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-6abdfba0fe1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest_input_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'BAD'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlinear_classifier_base\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_classifier_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_input_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1173\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1201\u001b[0m           self._get_features_and_labels_from_input_fn(input_fn, ModeKeys.TRAIN))\n\u001b[1;32m   1202\u001b[0m       \u001b[0mworker_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m       estimator_spec = self._call_model_fn(features, labels, ModeKeys.TRAIN,\n\u001b[0m\u001b[1;32m   1204\u001b[0m                                            self.config)\n\u001b[1;32m   1205\u001b[0m       \u001b[0mglobal_step_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1163\u001b[0;31m     \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1164\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/linear.py\u001b[0m in \u001b[0;36m_model_fn\u001b[0;34m(features, labels, mode, config)\u001b[0m\n\u001b[1;32m    935\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m       \u001b[0;34m\"\"\"Call the defined shared _linear_model_fn.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m       return _linear_model_fn_v2(\n\u001b[0m\u001b[1;32m    938\u001b[0m           \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m           \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/linear.py\u001b[0m in \u001b[0;36m_linear_model_fn_v2\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    663\u001b[0m                           optimizer)\n\u001b[1;32m    664\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 665\u001b[0;31m     logits, trainable_variables = _linear_model_fn_builder_v2(\n\u001b[0m\u001b[1;32m    666\u001b[0m         \u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits_dimension\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0mfeature_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_columns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/linear.py\u001b[0m in \u001b[0;36m_linear_model_fn_builder_v2\u001b[0;34m(units, feature_columns, sparse_combiner, features)\u001b[0m\n\u001b[1;32m    600\u001b[0m       \u001b[0msparse_combiner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msparse_combiner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       name='linear/linear_model')\n\u001b[0;32m--> 602\u001b[0;31m   \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m   \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_v1.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    774\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m               \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOperatorNotAllowedInGraphError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStagingError\u001b[0m: in user code:\n\n    /opt/conda/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/linear.py:1671 call  *\n        return self.layer(features)\n    /opt/conda/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/linear.py:1499 call  *\n        weighted_sum = fc_v2._create_weighted_sum(  # pylint: disable=protected-access\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/feature_column/feature_column_v2.py:2140 _create_weighted_sum  **\n        return _create_dense_column_weighted_sum(\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/feature_column/feature_column_v2.py:2150 _create_dense_column_weighted_sum\n        tensor = column.get_dense_tensor(transformation_cache, state_manager)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/feature_column/feature_column_v2.py:2592 get_dense_tensor\n        return transformation_cache.get(self, state_manager)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/feature_column/feature_column_v2.py:2355 get\n        transformed = column.transform_feature(self, state_manager)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/feature_column/feature_column_v2.py:2565 transform_feature\n        return self._transform_input_tensor(input_tensor)\n    /opt/conda/lib/python3.8/site-packages/tensorflow/python/feature_column/feature_column_v2.py:2539 _transform_input_tensor\n        input_tensor = self.normalizer_fn(input_tensor)\n    <ipython-input-14-9718c5439d32>:80 normalizer_fn\n        mean = normalize_parameters[column]['mean']\n\n    KeyError: <tf.Tensor 'linear/linear_model/linear/linear_model/linear/linear_model/CLAGE/ExpandDims:0' shape=(5, 1) dtype=float32>\n"
     ]
    }
   ],
   "source": [
    "train_input_fn = get_dataset(data_train, 'BAD', batch_size=500, mode='train')\n",
    "test_input_fn = get_dataset(data_test, 'BAD', batch_size=500)\n",
    "\n",
    "linear_classifier_base = linear_classifier_base.train(input_fn=train_input_fn, steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = linear_classifier_base.evaluate(input_fn=test_input_fn, steps=10)\n",
    "\n",
    "for key, value in metrics.items():\n",
    "    print(key, \":\", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate():\n",
    "    '''Remember to parametrize'''\n",
    "    # Get dataset\n",
    "    train_input_fn = get_dataset(data_train, 'BAD', batch_size=500)\n",
    "    test_input_fn = get_dataset(data_test, 'BAD', batch_size=500, repeat=False, shuffle=False)\n",
    "    # Get Features\n",
    "    feature_columns = get_features(NUMERICAL_VARIABLES, CATEGORICAL_VARIABLES, labels_dict)\n",
    "    # Get estimator\n",
    "    estimator = build_estimator(feature_columns)\n",
    "    # Train the estimator\n",
    "    estimator_train = estimator.train(input_fn=train_input_fn, steps=10)\n",
    "    # Evaluate \n",
    "    metrics = estimator_train.evaluate(input_fn=test_input_fn, steps=10)\n",
    "    return estimator_train, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, metrics = train_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create estimator train and evaluate function\n",
    "# def train_and_evaluate(args):\n",
    "#     tf.compat.v1.summary.FileWriterCache.clear() # ensure filewriter cache is clear for TensorBoard events file\n",
    "#     estimator = build_estimator(args['output_dir'], args['nbuckets'], args['hidden_units'].split(' '))\n",
    "#     train_spec = tf.estimator.TrainSpec(\n",
    "#         input_fn = read_dataset(\n",
    "#             filename = args['train_data_paths'],\n",
    "#             mode = tf.estimator.ModeKeys.TRAIN,\n",
    "#             batch_size = args['train_batch_size']),\n",
    "#         max_steps = args['train_steps'])\n",
    "#     exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n",
    "#     eval_spec = tf.estimator.EvalSpec(\n",
    "#         input_fn = read_dataset(\n",
    "#             filename = args['eval_data_paths'],\n",
    "#             mode = tf.estimator.ModeKeys.EVAL,\n",
    "#             batch_size = args['eval_batch_size']),\n",
    "#         steps = 100,\n",
    "#         exporters = exporter)\n",
    "#     tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example of predictions\n",
    "for pred in linear_classifier_base.predict(test_input_fn):\n",
    "    for key, value in pred.items():\n",
    "        print(key, \":\", value)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 1\n",
    "export_path = os.path.join(MODEL_DIR, str(version))\n",
    "os.rmdir(export_path)\n",
    "os.mkdir(export_path, mode=777)\n",
    "print('export_path = {}\\n'.format(export_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {}\n",
    "for feature in feature_columns:\n",
    "    inputs = {**inputs, **tf.feature_column.make_parse_example_spec([feature])}\n",
    "    serving_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn({**inputs, **tf.feature_column.make_parse_example_spec([feature])})\n",
    "estimator_path = estimator.export_saved_model(export_path, serving_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n",
    "  tf.feature_column.make_parse_example_spec([input_column]))\n",
    "estimator_base_path = os.path.join(tmpdir, 'from_estimator')\n",
    "estimator_path = estimator.export_saved_model(estimator_base_path, serving_input_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
