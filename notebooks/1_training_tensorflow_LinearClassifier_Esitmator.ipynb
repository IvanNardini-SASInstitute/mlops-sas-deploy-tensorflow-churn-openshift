{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "This notebook provides an example of how to train Tensorflow classifiers using the HMEQ dataset\n",
    "\n",
    "The goal is to predict whether a customer is a BAD (default) borrower, which in this dataset is a binary classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumption\n",
    "\n",
    "We are working in big data context. \n",
    "\n",
    "Then, I'm going to work with HMEQ dataset as it is so large that it would not fit in RAM. \n",
    "\n",
    "Then we use the Tensorflow framework to deal with that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#General\n",
    "import os\n",
    "import functools\n",
    "import pprint\n",
    "import tempfile\n",
    "\n",
    "#Analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR_PATH = os.getcwd()\n",
    "DATA_DIR_PATH = os.path.join(BASE_DIR_PATH, '../data')\n",
    "\n",
    "# Data directories paths\n",
    "TRAIN_DIR_PATH = os.path.join(DATA_DIR_PATH, 'train')\n",
    "TEST_DIR_PATH = os.path.join(DATA_DIR_PATH, 'test')\n",
    "VAL_DIR_PATH = os.path.join(DATA_DIR_PATH, 'val')\n",
    "\n",
    "# Data file paths\n",
    "TRAIN_DATA_PATH = os.path.join(TRAIN_DIR_PATH, 'train.csv')\n",
    "TEST_DATA_PATH = os.path.join(TEST_DIR_PATH, 'test.csv')\n",
    "VAL_DATA_PATH = os.path.join(VAL_DIR_PATH, 'val.csv')\n",
    "\n",
    "# Modeldir\n",
    "MODEL_DIR = os.path.join(BASE_DIR_PATH, '../models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing data\n",
    "def _set_categorical_type(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''Set the categorical type as string if needed'''\n",
    "    for column in CATEGORICAL_VARIABLES:\n",
    "        if (dataframe[column].dtype == 'O'):\n",
    "            dataframe[column] = dataframe[column].astype('string')\n",
    "    return dataframe\n",
    "\n",
    "def _set_categorical_empty(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''Change object type for categorical variable to avoid TF issue '''\n",
    "    for column in CATEGORICAL_VARIABLES:\n",
    "        if any(dataframe[column].isna()):\n",
    "            dataframe[column] = dataframe[column].fillna('')\n",
    "    return dataframe\n",
    "\n",
    "def _set_numerical_type(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''Set the numerical type as float64 if needed'''\n",
    "    for column in NUMERICAL_VARIABLES:\n",
    "        if (dataframe[column].dtype == 'int64'):\n",
    "            dataframe[column] = dataframe[column].astype('float64')\n",
    "    return dataframe\n",
    "\n",
    "def _get_impute_parameters_cat(categorical_variables: list) -> dict:\n",
    "    '''For each column in the numerical features, calculate mean.'''\n",
    "    impute_parameters = {}\n",
    "    for column in categorical_variables:\n",
    "        impute_parameters[column] = 'Missing'\n",
    "    return impute_parameters\n",
    "    \n",
    "def _impute_missing_categorical(inputs: dict, target) -> dict:\n",
    "    impute_parameters = _get_impute_parameters_cat(CATEGORICAL_VARIABLES)\n",
    "    # Since we modify just some features, \n",
    "    # we need to start by setting `outputs` to a copy of `inputs.\n",
    "    output = inputs.copy()\n",
    "    for key, value in impute_parameters.items():\n",
    "        is_blank = tf.math.equal('', inputs[key])\n",
    "        tf_other = tf.constant(value, dtype=np.string_)\n",
    "        output[key] = tf.where(is_blank, tf_other, inputs[key])\n",
    "    return output, target\n",
    "\n",
    "def _get_mean_parameter(dataframe: pd.DataFrame, column: str) -> float:\n",
    "    ''' Given a column, calculate mean'''\n",
    "    mean = dataframe[column].mean()\n",
    "    return mean\n",
    "\n",
    "def _get_impute_parameters_num(dataframe: pd.DataFrame, numerical_variables: list) -> dict:\n",
    "    '''For each column in the numerical features, calculate mean.'''\n",
    "    impute_parameters = {}\n",
    "    for column in numerical_variables:\n",
    "        impute_parameters[column] = _get_mean_parameter(dataframe, column)\n",
    "    return impute_parameters\n",
    "\n",
    "def _impute_missing_numerical(inputs: dict, target) -> dict:\n",
    "    '''Impute missing based on training mean'''\n",
    "    impute_parameters = _get_impute_parameters_num(data_train, NUMERICAL_VARIABLES) ## Here we have data_train\n",
    "    # Since we modify just some features, \n",
    "    # we need to start by setting `outputs` to a copy of `inputs.\n",
    "    output = inputs.copy()\n",
    "    for key, value in impute_parameters.items():\n",
    "        is_miss = tf.math.is_nan(inputs[key])\n",
    "        tf_mean = tf.constant(value, dtype=np.float64)\n",
    "        output[key] = tf.where(is_miss, tf_mean, inputs[key])\n",
    "    return output, target\n",
    "            \n",
    "# A utility method to create a feature column\n",
    "# and to transform a batch of data\n",
    "def check_feature(feature_column):\n",
    "    feature_layer = layers.DenseFeatures(feature_column)\n",
    "    print(feature_layer(example_batch).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 5 ../data/train/train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(TRAIN_DATA_PATH, sep=',')\n",
    "data_test = pd.read_csv(TEST_DATA_PATH, sep=',')\n",
    "data_val = pd.read_csv(VAL_DATA_PATH, sep=',')\n",
    "\n",
    "data_train.head(5)                                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: We notice that several variables (numerical and categorical) have missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data in Tensorflow\n",
    "\n",
    "Based on what I understood when you import data in Tensorflow you need two elements:\n",
    "\n",
    "**1. input_fn**: specifies how data is converted to a tf.data.Dataset that feeds the input pipeline.\n",
    "\n",
    "**2. feature column**: a construct that indicates a feature's data type.\n",
    "\n",
    "In our case, we notice that variables have missing. Then we need to impute them. Also we need to normalize data. \n",
    "\n",
    "And because we want to use Tensorflow framework, we can implement data preprocessing and transformation operations in the TensorFlow model itself. In this way, **it becomes an integral part of the model when the model is exported and deployed for predictions.**\n",
    "\n",
    "TensorFlow transformations can be accomplished in one of the following ways:\n",
    "\n",
    "1. Extending your base feature_columns (using crossed_column, embedding_column, bucketized_column, and so on).\n",
    "\n",
    "2. Implementing all of the instance-level transformation logic in a function that you call in all three input functions: train_input_fn, eval_input_fn, and serving_input_fn.\n",
    "\n",
    "3. If you are creating custom estimators, putting the code in the model_fn function.\n",
    "\n",
    "Then, we have two approaches to inputs:\n",
    "\n",
    "**1. Inside the input_fn**\n",
    "\n",
    "**2. While creating feature_column**\n",
    "\n",
    "Personally I prefer \n",
    "\n",
    "1. Preprocess data in the input_fn \n",
    "\n",
    "2. Do feature engineering while creating feature_column.\n",
    "\n",
    "About **the Data preprocessing strategy of impute missings**, \n",
    "\n",
    "- numerical variables: impute with mean\n",
    "\n",
    "- categorical variables: create 'other' class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define input_fn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = ['BAD']\n",
    "CATEGORICAL_VARIABLES = ['REASON', 'JOB']\n",
    "NUMERICAL_VARIABLES = ['LOAN', 'MORTDUE', 'VALUE', 'YOJ', 'DEROG', 'DELINQ', 'CLAGE', 'NINQ', 'CLNO', 'DEBTINC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the input function to load data into dataset\n",
    "\n",
    "def get_dataset(dataframe:pd.DataFrame, target:str, num_epochs=2, repeat=True, shuffle=True, batch_size=5, prefetch=True):\n",
    "    \n",
    "    def input_fn():\n",
    "        '''input_fn to read the data and impute missings'''\n",
    "        \n",
    "        # Extract\n",
    "        df = _set_categorical_type(dataframe)\n",
    "        df = _set_categorical_empty(df)\n",
    "        df = _set_numerical_type(df)\n",
    "        predictors = dict(df)\n",
    "        label = predictors.pop(target)\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((predictors, label))\n",
    "        \n",
    "        #Transform\n",
    "        dataset = dataset.map(_impute_missing_categorical)\n",
    "        dataset = dataset.map(_impute_missing_numerical)\n",
    "        if repeat:\n",
    "            dataset = dataset.repeat(num_epochs) # repeat the original dataset 3 times \n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(buffer_size=1000, seed=8) # shuffle with a buffer of 1000 element\n",
    "        dataset = dataset.batch(5, drop_remainder=True) # small batch size to print result\n",
    "        \n",
    "        #Load\n",
    "        if prefetch:\n",
    "            dataset = dataset.prefetch(1) #just to use it. It optimize training parallelizing batch loading over CPU and GPU\n",
    "            \n",
    "        #Load: to check\n",
    "        return dataset\n",
    "    \n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = get_dataset(data_train, 'BAD')\n",
    "test_input_fn = get_dataset(data_test, 'BAD', shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_batch, label_batch in train_input_fn().take(1):\n",
    "    print('Feature keys:', list(feature_batch.keys()))\n",
    "    print('A batch of REASON:', feature_batch['REASON'].numpy())\n",
    "    print('A batch of Labels:', label_batch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define features and configures feature_columns\n",
    "\n",
    "In order to import our training data into TensorFlow, we need to specify what type of data each feature contains. \n",
    "\n",
    "In our case, we have:\n",
    "\n",
    "1. **Categorical Data**: 'REASON', 'JOB'\n",
    "\n",
    "2. **Numerical Data**: 'LOAN', 'MORTDUE', 'VALUE', 'YOJ', 'DEROG', 'DELINQ', 'CLAGE', 'NINQ', 'CLNO', 'DEBTINC'\n",
    "\n",
    "In TensorFlow, we indicate a feature's data type using a construct called a **feature column**. \n",
    "\n",
    "Feature columns store only a description of the feature data; they do not contain the feature data itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_input_fn()            \n",
    "# We will use this batch to demonstrate several types of feature columns\n",
    "example_batch = next(iter(train_dataset))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/feature_column/numeric_column\n",
    "\n",
    "# Numerical variables\n",
    "\n",
    "for col_name in NUMERICAL_VARIABLES:\n",
    "    num_feature = tf.feature_column.numeric_column(col_name, dtype=tf.float64)\n",
    "    feature_columns.append(num_feature)\n",
    "    \n",
    "check_feature(feature_columns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical variables\n",
    "\n",
    "labels_dict= {'REASON': ['DebtCon', 'HomeImp', 'Missing'],\n",
    "              'JOB' : ['Other', 'Sales', 'ProfExe', 'Office', 'Mgr', 'Self', 'Missing']}\n",
    "\n",
    "for col_name in CATEGORICAL_VARIABLES:\n",
    "    cat_feature = tf.feature_column.categorical_column_with_vocabulary_list(col_name, labels_dict[col_name])\n",
    "    indicator_column = tf.feature_column.indicator_column(cat_feature)\n",
    "    feature_columns.append(indicator_column)\n",
    "\n",
    "check_feature(feature_columns[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a get_features function\n",
    "def get_features(num_features: list, cat_features: list, labels_dict:dict) -> list:\n",
    "    \n",
    "    # Create an empty list for feature\n",
    "    feature_columns = []\n",
    "    \n",
    "    #Get numerical features\n",
    "    for col_name in num_features:\n",
    "        num_feature = tf.feature_column.numeric_column(col_name, dtype=tf.float64)\n",
    "        feature_columns.append(num_feature)\n",
    "    \n",
    "    #Get categorical features\n",
    "    for col_name in cat_features:\n",
    "        cat_feature = tf.feature_column.categorical_column_with_vocabulary_list(col_name, labels_dict[col_name])\n",
    "        indicator_column = tf.feature_column.indicator_column(cat_feature)\n",
    "        feature_columns.append(indicator_column)\n",
    "        \n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = get_features(NUMERICAL_VARIABLES, CATEGORICAL_VARIABLES, labels_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Feature Layer\n",
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Base Estimator classifier\n",
    "# model_dir = tempfile.mkdtemp()\n",
    "os.mkdir('./test')\n",
    "modeldir = './test'\n",
    "linear_classifier_base = tf.estimator.LinearClassifier(\n",
    "    model_dir=modeldir, \n",
    "    feature_columns=feature_columns,\n",
    "    n_classes=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_estimator(feature_columns, learning_rate=0.1):\n",
    "    \"\"\"\n",
    "     Build an estimator.\n",
    "     \n",
    "    \"\"\"\n",
    "    feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
    "    \n",
    "#     model_dir = tempfile.mkdtemp() #make temporary cause we're going to use tensorboard\n",
    "    os.rmdir('./test')\n",
    "    os.mkdir('./test')\n",
    "    modeldir = './test'\n",
    "    runconfig = tf.estimator.RunConfig(tf_random_seed=8)\n",
    "    \n",
    "    linear_classifier_base = tf.estimator.LinearClassifier(\n",
    "    model_dir=modeldir, \n",
    "    feature_columns=feature_columns,\n",
    "    n_classes=2,\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "    )\n",
    "    \n",
    "    return linear_classifier_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = build_estimator(feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = get_dataset(data_train, 'BAD', batch_size=500)\n",
    "test_input_fn = get_dataset(data_test, 'BAD', batch_size=500, repeat=False, shuffle=False)\n",
    "\n",
    "linear_classifier_base = linear_classifier_base.train(input_fn=train_input_fn, steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = linear_classifier_base.evaluate(input_fn=test_input_fn, steps=10)\n",
    "\n",
    "for key, value in metrics.items():\n",
    "    print(key, \":\", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate():\n",
    "    '''Remember to parametrize'''\n",
    "    # Get dataset\n",
    "    train_input_fn = get_dataset(data_train, 'BAD', batch_size=500)\n",
    "    test_input_fn = get_dataset(data_test, 'BAD', batch_size=500, repeat=False, shuffle=False)\n",
    "    # Get Features\n",
    "    feature_columns = get_features(NUMERICAL_VARIABLES, CATEGORICAL_VARIABLES, labels_dict)\n",
    "    # Get estimator\n",
    "    estimator = build_estimator(feature_columns)\n",
    "    # Train the estimator\n",
    "    estimator_train = estimator.train(input_fn=train_input_fn, steps=10)\n",
    "    # Evaluate \n",
    "    metrics = estimator_train.evaluate(input_fn=test_input_fn, steps=10)\n",
    "    return estimator_train, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, metrics = train_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create estimator train and evaluate function\n",
    "# def train_and_evaluate(args):\n",
    "#     tf.compat.v1.summary.FileWriterCache.clear() # ensure filewriter cache is clear for TensorBoard events file\n",
    "#     estimator = build_estimator(args['output_dir'], args['nbuckets'], args['hidden_units'].split(' '))\n",
    "#     train_spec = tf.estimator.TrainSpec(\n",
    "#         input_fn = read_dataset(\n",
    "#             filename = args['train_data_paths'],\n",
    "#             mode = tf.estimator.ModeKeys.TRAIN,\n",
    "#             batch_size = args['train_batch_size']),\n",
    "#         max_steps = args['train_steps'])\n",
    "#     exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n",
    "#     eval_spec = tf.estimator.EvalSpec(\n",
    "#         input_fn = read_dataset(\n",
    "#             filename = args['eval_data_paths'],\n",
    "#             mode = tf.estimator.ModeKeys.EVAL,\n",
    "#             batch_size = args['eval_batch_size']),\n",
    "#         steps = 100,\n",
    "#         exporters = exporter)\n",
    "#     tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example of predictions\n",
    "for pred in linear_classifier_base.predict(test_input_fn):\n",
    "    for key, value in pred.items():\n",
    "        print(key, \":\", value)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 1\n",
    "export_path = os.path.join(MODEL_DIR, str(version))\n",
    "os.rmdir(export_path)\n",
    "os.mkdir(export_path, mode=777)\n",
    "print('export_path = {}\\n'.format(export_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {}\n",
    "for feature in feature_columns:\n",
    "    inputs = {**inputs, **tf.feature_column.make_parse_example_spec([feature])}\n",
    "    serving_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn({**inputs, **tf.feature_column.make_parse_example_spec([feature])})\n",
    "estimator_path = estimator.export_saved_model(export_path, serving_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n",
    "  tf.feature_column.make_parse_example_spec([input_column]))\n",
    "estimator_base_path = os.path.join(tmpdir, 'from_estimator')\n",
    "estimator_path = estimator.export_saved_model(estimator_base_path, serving_input_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
