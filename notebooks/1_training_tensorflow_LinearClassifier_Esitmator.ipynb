{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "This notebook provides an example of how to train Tensorflow classifiers using the HMEQ dataset\n",
    "\n",
    "The goal is to predict whether a customer is a BAD (default) borrower, which in this dataset is a binary classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumption\n",
    "\n",
    "We are working in big data context. \n",
    "\n",
    "Then, I'm going to work with HMEQ dataset as it is so large that it would not fit in RAM. \n",
    "\n",
    "Then we use the Tensorflow framework to deal with that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#General\n",
    "import os\n",
    "import functools\n",
    "import shutil\n",
    "import datetime\n",
    "import pprint\n",
    "import tempfile\n",
    "\n",
    "#Analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR_PATH = os.getcwd()\n",
    "DATA_DIR_PATH = os.path.join(BASE_DIR_PATH, '../data')\n",
    "\n",
    "# Data directories paths\n",
    "TRAIN_DIR_PATH = os.path.join(DATA_DIR_PATH, 'train')\n",
    "TEST_DIR_PATH = os.path.join(DATA_DIR_PATH, 'test')\n",
    "VAL_DIR_PATH = os.path.join(DATA_DIR_PATH, 'val')\n",
    "\n",
    "# Data file paths\n",
    "TRAIN_DATA_PATH = os.path.join(TRAIN_DIR_PATH, 'train.csv')\n",
    "TEST_DATA_PATH = os.path.join(TEST_DIR_PATH, 'test.csv')\n",
    "VAL_DATA_PATH = os.path.join(VAL_DIR_PATH, 'val.csv')\n",
    "\n",
    "# Model directories\n",
    "LOG_DIR = os.path.join(BASE_DIR_PATH, '../logs')\n",
    "MODEL_DIR = os.path.join(BASE_DIR_PATH, '../models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing data\n",
    "def _set_categorical_type(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''Set the categorical type as string if needed'''\n",
    "    for column in CATEGORICAL_VARIABLES:\n",
    "        if (dataframe[column].dtype == 'O'):\n",
    "            dataframe[column] = dataframe[column].astype('string')\n",
    "    return dataframe\n",
    "\n",
    "def _set_categorical_empty(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''Change object type for categorical variable to avoid TF issue '''\n",
    "    for column in CATEGORICAL_VARIABLES:\n",
    "        if any(dataframe[column].isna()):\n",
    "            dataframe[column] = dataframe[column].fillna('')\n",
    "    return dataframe\n",
    "\n",
    "def _set_numerical_type(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''Set the numerical type as float64 if needed'''\n",
    "    for column in NUMERICAL_VARIABLES:\n",
    "        if (dataframe[column].dtype == 'int64'):\n",
    "            dataframe[column] = dataframe[column].astype('float64')\n",
    "    return dataframe\n",
    "\n",
    "def _get_impute_parameters_cat(categorical_variables: list) -> dict:\n",
    "    '''For each column in the numerical features, calculate mean.'''\n",
    "    impute_parameters = {}\n",
    "    for column in categorical_variables:\n",
    "        impute_parameters[column] = 'Missing'\n",
    "    return impute_parameters\n",
    "    \n",
    "def _impute_missing_categorical(inputs: dict, target) -> dict:\n",
    "    impute_parameters = _get_impute_parameters_cat(CATEGORICAL_VARIABLES)\n",
    "    # Since we modify just some features, \n",
    "    # we need to start by setting `outputs` to a copy of `inputs.\n",
    "    output = inputs.copy()\n",
    "    for key, value in impute_parameters.items():\n",
    "        is_blank = tf.math.equal('', inputs[key])\n",
    "        tf_other = tf.constant(value, dtype=np.string_)\n",
    "        output[key] = tf.where(is_blank, tf_other, inputs[key])\n",
    "    return output, target\n",
    "\n",
    "def _get_mean_parameter(dataframe: pd.DataFrame, column: str) -> float:\n",
    "    ''' Given a column, calculate mean'''\n",
    "    mean = dataframe[column].mean()\n",
    "    return mean\n",
    "\n",
    "def _get_impute_parameters_num(dataframe: pd.DataFrame, numerical_variables: list) -> dict:\n",
    "    '''For each column in the numerical features, calculate mean.'''\n",
    "    impute_parameters = {}\n",
    "    for column in numerical_variables:\n",
    "        impute_parameters[column] = _get_mean_parameter(dataframe, column)\n",
    "    return impute_parameters\n",
    "\n",
    "def _impute_missing_numerical(inputs: dict, target) -> dict:\n",
    "    '''Impute missing based on training mean'''\n",
    "    impute_parameters = _get_impute_parameters_num(data_train, NUMERICAL_VARIABLES) ## Here we have data_train\n",
    "    # Since we modify just some features, \n",
    "    # we need to start by setting `outputs` to a copy of `inputs.\n",
    "    output = inputs.copy()\n",
    "    for key, value in impute_parameters.items():\n",
    "        is_miss = tf.math.is_nan(inputs[key])\n",
    "        tf_mean = tf.constant(value, dtype=np.float64)\n",
    "        output[key] = tf.where(is_miss, tf_mean, inputs[key])\n",
    "    return output, target\n",
    "\n",
    "def _get_std_parameter(dataframe:pd.DataFrame, column:str) -> float:\n",
    "    '''Given a column, calculate sd'''\n",
    "    std = dataframe[column].std()\n",
    "    return std\n",
    "\n",
    "def _get_normalization_parameters(numerical_variables: list) -> dict:\n",
    "    '''For each column in the numerical features, calculate mean and std.'''\n",
    "    normalize_parameters = {}\n",
    "    for column in numerical_variables:\n",
    "        normalize_parameters[column] = {}\n",
    "        normalize_parameters[column]['mean'] = _get_mean_parameter(data_train, column)\n",
    "        normalize_parameters[column]['std'] = _get_std_parameter(data_train, column)\n",
    "    return normalize_parameters\n",
    "    \n",
    "def normalizer(column, mean, std):\n",
    "    return (column - mean) / std\n",
    "            \n",
    "# A utility method to create a feature column\n",
    "# and to transform a batch of data\n",
    "def check_feature(feature_column):\n",
    "    feature_layer = layers.DenseFeatures(feature_column)\n",
    "    print(feature_layer(example_batch).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAD,LOAN,MORTDUE,VALUE,REASON,JOB,YOJ,DEROG,DELINQ,CLAGE,NINQ,CLNO,DEBTINC\r\n",
      "0,34400,97971.0,145124.0,DebtCon,Other,13.0,0.0,0.0,67.8320416646805,1.0,36.0,40.4027058419691\r\n",
      "0,13600,89937.0,110986.0,DebtCon,Sales,14.0,,2.0,146.718742448452,1.0,17.0,33.7471158335903\r\n",
      "1,10800,75000.0,87400.0,HomeImp,Other,7.0,1.0,0.0,101.46666666666701,2.0,19.0,\r\n",
      "0,14900,87167.0,114219.0,DebtCon,ProfExe,8.0,0.0,0.0,194.113173533089,2.0,36.0,41.3297639035293\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 5 ../data/train/train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BAD</th>\n",
       "      <th>LOAN</th>\n",
       "      <th>MORTDUE</th>\n",
       "      <th>VALUE</th>\n",
       "      <th>REASON</th>\n",
       "      <th>JOB</th>\n",
       "      <th>YOJ</th>\n",
       "      <th>DEROG</th>\n",
       "      <th>DELINQ</th>\n",
       "      <th>CLAGE</th>\n",
       "      <th>NINQ</th>\n",
       "      <th>CLNO</th>\n",
       "      <th>DEBTINC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>34400</td>\n",
       "      <td>97971.0</td>\n",
       "      <td>145124.0</td>\n",
       "      <td>DebtCon</td>\n",
       "      <td>Other</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67.832042</td>\n",
       "      <td>1.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>40.402706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>13600</td>\n",
       "      <td>89937.0</td>\n",
       "      <td>110986.0</td>\n",
       "      <td>DebtCon</td>\n",
       "      <td>Sales</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>146.718742</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>33.747116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>10800</td>\n",
       "      <td>75000.0</td>\n",
       "      <td>87400.0</td>\n",
       "      <td>HomeImp</td>\n",
       "      <td>Other</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101.466667</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>14900</td>\n",
       "      <td>87167.0</td>\n",
       "      <td>114219.0</td>\n",
       "      <td>DebtCon</td>\n",
       "      <td>ProfExe</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>194.113174</td>\n",
       "      <td>2.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>41.329764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>7200</td>\n",
       "      <td>98691.0</td>\n",
       "      <td>115750.0</td>\n",
       "      <td>HomeImp</td>\n",
       "      <td>Office</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>118.000142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>37.720359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   BAD   LOAN  MORTDUE     VALUE   REASON      JOB   YOJ  DEROG  DELINQ  \\\n",
       "0    0  34400  97971.0  145124.0  DebtCon    Other  13.0    0.0     0.0   \n",
       "1    0  13600  89937.0  110986.0  DebtCon    Sales  14.0    NaN     2.0   \n",
       "2    1  10800  75000.0   87400.0  HomeImp    Other   7.0    1.0     0.0   \n",
       "3    0  14900  87167.0  114219.0  DebtCon  ProfExe   8.0    0.0     0.0   \n",
       "4    0   7200  98691.0  115750.0  HomeImp   Office  22.0    0.0     0.0   \n",
       "\n",
       "        CLAGE  NINQ  CLNO    DEBTINC  \n",
       "0   67.832042   1.0  36.0  40.402706  \n",
       "1  146.718742   1.0  17.0  33.747116  \n",
       "2  101.466667   2.0  19.0        NaN  \n",
       "3  194.113174   2.0  36.0  41.329764  \n",
       "4  118.000142   0.0  11.0  37.720359  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_csv(TRAIN_DATA_PATH, sep=',')\n",
    "data_test = pd.read_csv(TEST_DATA_PATH, sep=',')\n",
    "data_val = pd.read_csv(VAL_DATA_PATH, sep=',')\n",
    "\n",
    "data_train.head(5)                                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4827 entries, 0 to 4826\n",
      "Data columns (total 13 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   BAD      4827 non-null   int64  \n",
      " 1   LOAN     4827 non-null   int64  \n",
      " 2   MORTDUE  4405 non-null   float64\n",
      " 3   VALUE    4738 non-null   float64\n",
      " 4   REASON   4618 non-null   object \n",
      " 5   JOB      4601 non-null   object \n",
      " 6   YOJ      4419 non-null   float64\n",
      " 7   DEROG    4262 non-null   float64\n",
      " 8   DELINQ   4362 non-null   float64\n",
      " 9   CLAGE    4578 non-null   float64\n",
      " 10  NINQ     4421 non-null   float64\n",
      " 11  CLNO     4651 non-null   float64\n",
      " 12  DEBTINC  3804 non-null   float64\n",
      "dtypes: float64(9), int64(2), object(2)\n",
      "memory usage: 490.4+ KB\n"
     ]
    }
   ],
   "source": [
    "data_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BAD</th>\n",
       "      <td>4827.0</td>\n",
       "      <td>0.198674</td>\n",
       "      <td>0.399043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOAN</th>\n",
       "      <td>4827.0</td>\n",
       "      <td>18617.112078</td>\n",
       "      <td>11231.974061</td>\n",
       "      <td>1100.000000</td>\n",
       "      <td>11100.000000</td>\n",
       "      <td>16300.000000</td>\n",
       "      <td>23300.000000</td>\n",
       "      <td>89900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MORTDUE</th>\n",
       "      <td>4405.0</td>\n",
       "      <td>73796.339364</td>\n",
       "      <td>43741.460123</td>\n",
       "      <td>2063.000000</td>\n",
       "      <td>46884.000000</td>\n",
       "      <td>65206.000000</td>\n",
       "      <td>91491.000000</td>\n",
       "      <td>399412.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VALUE</th>\n",
       "      <td>4738.0</td>\n",
       "      <td>101633.021895</td>\n",
       "      <td>56564.609914</td>\n",
       "      <td>8000.000000</td>\n",
       "      <td>66260.250000</td>\n",
       "      <td>89407.500000</td>\n",
       "      <td>119732.500000</td>\n",
       "      <td>855909.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YOJ</th>\n",
       "      <td>4419.0</td>\n",
       "      <td>8.957377</td>\n",
       "      <td>7.604500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>41.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEROG</th>\n",
       "      <td>4262.0</td>\n",
       "      <td>0.244486</td>\n",
       "      <td>0.823733</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DELINQ</th>\n",
       "      <td>4362.0</td>\n",
       "      <td>0.446813</td>\n",
       "      <td>1.138853</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLAGE</th>\n",
       "      <td>4578.0</td>\n",
       "      <td>179.902913</td>\n",
       "      <td>86.744368</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>114.858318</td>\n",
       "      <td>173.497696</td>\n",
       "      <td>231.876088</td>\n",
       "      <td>1168.233561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NINQ</th>\n",
       "      <td>4421.0</td>\n",
       "      <td>1.199276</td>\n",
       "      <td>1.745287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLNO</th>\n",
       "      <td>4651.0</td>\n",
       "      <td>21.322081</td>\n",
       "      <td>10.111162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>71.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEBTINC</th>\n",
       "      <td>3804.0</td>\n",
       "      <td>33.768804</td>\n",
       "      <td>8.806656</td>\n",
       "      <td>0.524499</td>\n",
       "      <td>29.149239</td>\n",
       "      <td>34.818353</td>\n",
       "      <td>38.972750</td>\n",
       "      <td>203.312149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          count           mean           std          min           25%  \\\n",
       "BAD      4827.0       0.198674      0.399043     0.000000      0.000000   \n",
       "LOAN     4827.0   18617.112078  11231.974061  1100.000000  11100.000000   \n",
       "MORTDUE  4405.0   73796.339364  43741.460123  2063.000000  46884.000000   \n",
       "VALUE    4738.0  101633.021895  56564.609914  8000.000000  66260.250000   \n",
       "YOJ      4419.0       8.957377      7.604500     0.000000      3.000000   \n",
       "DEROG    4262.0       0.244486      0.823733     0.000000      0.000000   \n",
       "DELINQ   4362.0       0.446813      1.138853     0.000000      0.000000   \n",
       "CLAGE    4578.0     179.902913     86.744368     0.000000    114.858318   \n",
       "NINQ     4421.0       1.199276      1.745287     0.000000      0.000000   \n",
       "CLNO     4651.0      21.322081     10.111162     0.000000     15.000000   \n",
       "DEBTINC  3804.0      33.768804      8.806656     0.524499     29.149239   \n",
       "\n",
       "                  50%            75%            max  \n",
       "BAD          0.000000       0.000000       1.000000  \n",
       "LOAN     16300.000000   23300.000000   89900.000000  \n",
       "MORTDUE  65206.000000   91491.000000  399412.000000  \n",
       "VALUE    89407.500000  119732.500000  855909.000000  \n",
       "YOJ          7.000000      13.000000      41.000000  \n",
       "DEROG        0.000000       0.000000      10.000000  \n",
       "DELINQ       0.000000       0.000000      15.000000  \n",
       "CLAGE      173.497696     231.876088    1168.233561  \n",
       "NINQ         1.000000       2.000000      17.000000  \n",
       "CLNO        20.000000      26.000000      71.000000  \n",
       "DEBTINC     34.818353      38.972750     203.312149  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.801326\n",
       "1    0.198674\n",
       "Name: BAD, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['BAD'].value_counts()/data_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: We notice that several variables (numerical and categorical) have missing values. The dataset is unbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data in Tensorflow\n",
    "\n",
    "Based on what I understood when you import data in Tensorflow you need two elements:\n",
    "\n",
    "**1. input_fn**: specifies how data is converted to a tf.data.Dataset that feeds the input pipeline.\n",
    "\n",
    "**2. feature column**: a construct that indicates a feature's data type.\n",
    "\n",
    "In our case, we notice that variables have missing. Then we need to impute them. Also we need to normalize data. \n",
    "\n",
    "And because we want to use Tensorflow framework, we can implement data preprocessing and transformation operations in the TensorFlow model itself. In this way, **it becomes an integral part of the model when the model is exported and deployed for predictions.**\n",
    "\n",
    "TensorFlow transformations can be accomplished in one of the following ways:\n",
    "\n",
    "1. Extending your base feature_columns (using crossed_column, embedding_column, bucketized_column, and so on).\n",
    "\n",
    "2. Implementing all of the instance-level transformation logic in a function that you call in all three input functions: train_input_fn, eval_input_fn, and serving_input_fn.\n",
    "\n",
    "3. If you are creating custom estimators, putting the code in the model_fn function.\n",
    "\n",
    "Then, we have two approaches to inputs:\n",
    "\n",
    "**1. Inside the input_fn**\n",
    "\n",
    "**2. While creating feature_column**\n",
    "\n",
    "Personally I prefer \n",
    "\n",
    "1. Preprocess data in the input_fn \n",
    "\n",
    "2. Do feature engineering while creating feature_column.\n",
    "\n",
    "About **the Data preprocessing strategy of impute missings**, \n",
    "\n",
    "- numerical variables: impute with mean\n",
    "\n",
    "- categorical variables: create 'other' class\n",
    "\n",
    "About **Feature Engineering**, \n",
    "\n",
    "- define normalizer_fn to normalize numerical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define input_fn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = ['BAD']\n",
    "CATEGORICAL_VARIABLES = ['REASON', 'JOB']\n",
    "NUMERICAL_VARIABLES = ['LOAN', 'MORTDUE', 'VALUE', 'YOJ', 'DEROG', 'DELINQ', 'CLAGE', 'NINQ', 'CLNO', 'DEBTINC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the input function to load data into dataset\n",
    "\n",
    "def get_dataset(dataframe:pd.DataFrame, target:str, num_epochs=2, mode='eval', batch_size=5):\n",
    "    \n",
    "    def input_fn():\n",
    "        '''input_fn to read the data and impute missings'''\n",
    "        \n",
    "        # Extract\n",
    "        df = _set_categorical_type(dataframe)\n",
    "        df = _set_categorical_empty(df)\n",
    "        df = _set_numerical_type(df)\n",
    "        predictors = dict(df)\n",
    "        label = predictors.pop(target)\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((predictors, label))\n",
    "        \n",
    "        #Transform\n",
    "        dataset = dataset.map(_impute_missing_categorical)\n",
    "        dataset = dataset.map(_impute_missing_numerical)\n",
    "        \n",
    "        if mode == 'train':\n",
    "            dataset = dataset.repeat(num_epochs) # repeat the original dataset 3 times \n",
    "            dataset = dataset.shuffle(buffer_size=1000, seed=8) # shuffle with a buffer of 1000 element\n",
    "            \n",
    "        dataset = dataset.batch(5, drop_remainder=True) # small batch size to print result\n",
    "        \n",
    "        #Load\n",
    "        dataset = dataset.prefetch(1) #just to use it. It optimize training parallelizing batch loading over CPU and GPU\n",
    "            \n",
    "        #Load: to check\n",
    "        return dataset\n",
    "    \n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = get_dataset(data_train, 'BAD', mode='train')\n",
    "test_input_fn = get_dataset(data_test, 'BAD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature keys: ['LOAN', 'MORTDUE', 'VALUE', 'REASON', 'JOB', 'YOJ', 'DEROG', 'DELINQ', 'CLAGE', 'NINQ', 'CLNO', 'DEBTINC']\n",
      "A batch of REASON: [b'DebtCon' b'DebtCon' b'DebtCon' b'DebtCon' b'DebtCon']\n",
      "A batch of Labels: [0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "for feature_batch, label_batch in train_input_fn().take(1):\n",
    "    print('Feature keys:', list(feature_batch.keys()))\n",
    "    print('A batch of REASON:', feature_batch['REASON'].numpy())\n",
    "    print('A batch of Labels:', label_batch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define features and configures feature_columns\n",
    "\n",
    "In order to import our training data into TensorFlow, we need to specify what type of data each feature contains. \n",
    "\n",
    "In our case, we have:\n",
    "\n",
    "1. **Categorical Data**: 'REASON', 'JOB'\n",
    "\n",
    "2. **Numerical Data**: 'LOAN', 'MORTDUE', 'VALUE', 'YOJ', 'DEROG', 'DELINQ', 'CLAGE', 'NINQ', 'CLNO', 'DEBTINC'\n",
    "\n",
    "In TensorFlow, we indicate a feature's data type using a construct called a **feature column**. \n",
    "\n",
    "Feature columns store only a description of the feature data; they do not contain the feature data itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_input_fn()            \n",
    "# We will use this batch to demonstrate several types of feature columns\n",
    "example_batch = next(iter(train_dataset))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical variables\n",
    "\n",
    "normalize_parameters = _get_normalization_parameters(NUMERICAL_VARIABLES)\n",
    "\n",
    "for col_name in NUMERICAL_VARIABLES:\n",
    "    mean = normalize_parameters[col_name]['mean']\n",
    "    std = normalize_parameters[col_name]['std']\n",
    "    normalizer_fn = functools.partial(normalizer, mean=mean, std=std)\n",
    "    num_feature = tf.feature_column.numeric_column(col_name, dtype=tf.float64, normalizer_fn=normalizer_fn)\n",
    "    feature_columns.append(num_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dense_features is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "[[ 0.80866367]\n",
      " [-0.97196734]\n",
      " [ 0.03408917]\n",
      " [ 1.2805308 ]\n",
      " [-0.3309402 ]]\n"
     ]
    }
   ],
   "source": [
    "check_feature(feature_columns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical variables\n",
    "\n",
    "labels_dict= {'REASON': ['DebtCon', 'HomeImp', 'Missing'],\n",
    "              'JOB' : ['Other', 'Sales', 'ProfExe', 'Office', 'Mgr', 'Self', 'Missing']}\n",
    "\n",
    "for col_name in CATEGORICAL_VARIABLES:\n",
    "    cat_feature = tf.feature_column.categorical_column_with_vocabulary_list(col_name, labels_dict[col_name])\n",
    "    indicator_column = tf.feature_column.indicator_column(cat_feature)\n",
    "    feature_columns.append(indicator_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dense_features_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "[[1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "check_feature(feature_columns[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NumericColumn(key='LOAN', shape=(1,), default_value=None, dtype=tf.float64, normalizer_fn=functools.partial(<function normalizer at 0x7fe400cbe820>, mean=18617.112077895174, std=11231.974060952927)),\n",
       " NumericColumn(key='VALUE', shape=(1,), default_value=None, dtype=tf.float64, normalizer_fn=functools.partial(<function normalizer at 0x7fe400cbe820>, mean=101633.02189531446, std=56564.60991374346)),\n",
       " NumericColumn(key='DEROG', shape=(1,), default_value=None, dtype=tf.float64, normalizer_fn=functools.partial(<function normalizer at 0x7fe400cbe820>, mean=0.24448615673392773, std=0.8237334691118142)),\n",
       " NumericColumn(key='CLAGE', shape=(1,), default_value=None, dtype=tf.float64, normalizer_fn=functools.partial(<function normalizer at 0x7fe400cbe820>, mean=179.90291295790095, std=86.74436793774018)),\n",
       " NumericColumn(key='CLNO', shape=(1,), default_value=None, dtype=tf.float64, normalizer_fn=functools.partial(<function normalizer at 0x7fe400cbe820>, mean=21.32208127284455, std=10.11116196010631)),\n",
       " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='REASON', vocabulary_list=('DebtCon', 'HomeImp', 'Missing'), dtype=tf.string, default_value=-1, num_oov_buckets=0))]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show some examples\n",
    "feature_columns[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a get_features function\n",
    "def get_features(num_features: list, cat_features: list, labels_dict:dict) -> list:\n",
    "    \n",
    "    # Create an empty list for feature\n",
    "    feature_columns = []\n",
    "    \n",
    "    #Get numerical features\n",
    "    normalize_parameters = _get_normalization_parameters(num_features)\n",
    "    for col_name in num_features:\n",
    "        mean = normalize_parameters[col_name]['mean']\n",
    "        std = normalize_parameters[col_name]['std']\n",
    "        normalizer_fn = functools.partial(normalizer, mean=mean, std=std)\n",
    "        num_feature = tf.feature_column.numeric_column(col_name, dtype=tf.float64, normalizer_fn=normalizer_fn)\n",
    "        feature_columns.append(num_feature)\n",
    "    \n",
    "    #Get categorical features\n",
    "    for col_name in cat_features:\n",
    "        cat_feature = tf.feature_column.categorical_column_with_vocabulary_list(col_name, labels_dict[col_name])\n",
    "        indicator_column = tf.feature_column.indicator_column(cat_feature)\n",
    "        feature_columns.append(indicator_column)\n",
    "        \n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = get_features(NUMERICAL_VARIABLES, CATEGORICAL_VARIABLES, labels_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Feature Layer\n",
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Base Estimator classifier\n",
    "linear_classifier_base = tf.estimator.LinearClassifier(\n",
    "    model_dir=LOG_DIR, \n",
    "    feature_columns=feature_columns,\n",
    "    n_classes=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_estimator(feature_columns, learning_rate=0.1):\n",
    "    \"\"\"\n",
    "     Build an estimator.\n",
    "     \n",
    "    \"\"\"\n",
    "    feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
    "    \n",
    "    runconfig = tf.estimator.RunConfig(tf_random_seed=8)\n",
    "    \n",
    "    linear_classifier_base = tf.estimator.LinearClassifier(\n",
    "    model_dir=LOG_DIR, \n",
    "    feature_columns=feature_columns,\n",
    "    n_classes=2,\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "    )\n",
    "    \n",
    "    return linear_classifier_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/home/jovyan/work/notebooks/../logs', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "estimator = build_estimator(feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = get_dataset(data_train, 'BAD', batch_size=500, mode='train')\n",
    "test_input_fn = get_dataset(data_test, 'BAD', batch_size=500)\n",
    "\n",
    "linear_classifier_base = linear_classifier_base.train(input_fn=train_input_fn, steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = linear_classifier_base.evaluate(input_fn=test_input_fn, steps=10)\n",
    "\n",
    "for key, value in metrics.items():\n",
    "    print(key, \":\", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate():\n",
    "    '''Remember to parametrize'''\n",
    "    # Get dataset\n",
    "    train_input_fn = get_dataset(data_train, 'BAD', batch_size=500, mode='train')\n",
    "    test_input_fn = get_dataset(data_test, 'BAD', batch_size=500)\n",
    "    # Get Features\n",
    "    feature_columns = get_features(NUMERICAL_VARIABLES, CATEGORICAL_VARIABLES, labels_dict)\n",
    "    # Get estimator\n",
    "    estimator = build_estimator(feature_columns)\n",
    "    # Train the estimator\n",
    "    estimator_train = estimator.train(input_fn=train_input_fn, steps=10)\n",
    "    # Evaluate \n",
    "    metrics = estimator_train.evaluate(input_fn=test_input_fn, steps=10)\n",
    "    return estimator_train, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/home/jovyan/work/notebooks/../logs', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.8/site-packages/tensorflow/python/training/training_util.py:235: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer linear/linear_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/linear.py:1471: Layer.add_variable (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/jovyan/work/notebooks/../logs/model.ckpt-10\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.8/site-packages/tensorflow/python/training/saver.py:1077: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file utilities to get mtimes.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 10...\n",
      "INFO:tensorflow:Saving checkpoints for 10 into /home/jovyan/work/notebooks/../logs/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 10...\n",
      "INFO:tensorflow:loss = 0.3846243, step = 10\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 20...\n",
      "INFO:tensorflow:Saving checkpoints for 20 into /home/jovyan/work/notebooks/../logs/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 20...\n",
      "INFO:tensorflow:Loss for final step: 0.25045902.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer linear/linear_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2020-09-25T15:36:54Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/jovyan/work/notebooks/../logs/model.ckpt-20\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/10]\n",
      "INFO:tensorflow:Evaluation [2/10]\n",
      "INFO:tensorflow:Evaluation [3/10]\n",
      "INFO:tensorflow:Evaluation [4/10]\n",
      "INFO:tensorflow:Evaluation [5/10]\n",
      "INFO:tensorflow:Evaluation [6/10]\n",
      "INFO:tensorflow:Evaluation [7/10]\n",
      "INFO:tensorflow:Evaluation [8/10]\n",
      "INFO:tensorflow:Evaluation [9/10]\n",
      "INFO:tensorflow:Evaluation [10/10]\n",
      "INFO:tensorflow:Inference Time : 0.54072s\n",
      "INFO:tensorflow:Finished evaluation at 2020-09-25-15:36:54\n",
      "INFO:tensorflow:Saving dict for global step 20: accuracy = 0.86, accuracy_baseline = 0.86, auc = 0.4700997, auc_precision_recall = 0.18217665, average_loss = 0.4576046, global_step = 20, label/mean = 0.14, loss = 0.45760456, precision = 0.0, prediction/mean = 0.26048952, recall = 0.0\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 20: /home/jovyan/work/notebooks/../logs/model.ckpt-20\n"
     ]
    }
   ],
   "source": [
    "model, metrics = train_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create estimator train and evaluate function\n",
    "# def train_and_evaluate(args):\n",
    "#     tf.compat.v1.summary.FileWriterCache.clear() # ensure filewriter cache is clear for TensorBoard events file\n",
    "#     estimator = build_estimator(args['output_dir'], args['nbuckets'], args['hidden_units'].split(' '))\n",
    "#     train_spec = tf.estimator.TrainSpec(\n",
    "#         input_fn = read_dataset(\n",
    "#             filename = args['train_data_paths'],\n",
    "#             mode = tf.estimator.ModeKeys.TRAIN,\n",
    "#             batch_size = args['train_batch_size']),\n",
    "#         max_steps = args['train_steps'])\n",
    "#     exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n",
    "#     eval_spec = tf.estimator.EvalSpec(\n",
    "#         input_fn = read_dataset(\n",
    "#             filename = args['eval_data_paths'],\n",
    "#             mode = tf.estimator.ModeKeys.EVAL,\n",
    "#             batch_size = args['eval_batch_size']),\n",
    "#         steps = 100,\n",
    "#         exporters = exporter)\n",
    "#     tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example of predictions\n",
    "for pred in linear_classifier_base.predict(test_input_fn):\n",
    "    for key, value in pred.items():\n",
    "        print(key, \":\", value)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://guillaumegenthial.github.io/serving-tensorflow-estimator.html\n",
    "# shutil.rmtree(MODEL_DIR, ignore_errors=True)\n",
    "VERSION = 1\n",
    "DATE = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "ID = \"_\".join([str(DATE), str(VERSION)])\n",
    "EXPORT_PATH = os.path.join(MODEL_DIR, ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/work/notebooks/../models/20200925153703_1'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXPORT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export_path = os.path.join(MODEL_DIR, str(version))\n",
    "# os.rmdir(export_path)\n",
    "# os.mkdir(export_path, mode=777)\n",
    "# print('export_path = {}\\n'.format(export_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NumericColumn(key='LOAN', shape=(1,), default_value=None, dtype=tf.float64, normalizer_fn=functools.partial(<function normalizer at 0x7fe400cbe820>, mean=18617.112077895174, std=11231.974060952927)),\n",
       " NumericColumn(key='MORTDUE', shape=(1,), default_value=None, dtype=tf.float64, normalizer_fn=functools.partial(<function normalizer at 0x7fe400cbe820>, mean=73796.33936435868, std=43741.46012256415)),\n",
       " NumericColumn(key='VALUE', shape=(1,), default_value=None, dtype=tf.float64, normalizer_fn=functools.partial(<function normalizer at 0x7fe400cbe820>, mean=101633.02189531446, std=56564.60991374346)),\n",
       " NumericColumn(key='YOJ', shape=(1,), default_value=None, dtype=tf.float64, normalizer_fn=functools.partial(<function normalizer at 0x7fe400cbe820>, mean=8.957377234668478, std=7.60449983387714)),\n",
       " NumericColumn(key='DEROG', shape=(1,), default_value=None, dtype=tf.float64, normalizer_fn=functools.partial(<function normalizer at 0x7fe400cbe820>, mean=0.24448615673392773, std=0.8237334691118142)),\n",
       " NumericColumn(key='DELINQ', shape=(1,), default_value=None, dtype=tf.float64, normalizer_fn=functools.partial(<function normalizer at 0x7fe400cbe820>, mean=0.44681338835396606, std=1.1388534108985295)),\n",
       " NumericColumn(key='CLAGE', shape=(1,), default_value=None, dtype=tf.float64, normalizer_fn=functools.partial(<function normalizer at 0x7fe400cbe820>, mean=179.90291295790095, std=86.74436793774018)),\n",
       " NumericColumn(key='NINQ', shape=(1,), default_value=None, dtype=tf.float64, normalizer_fn=functools.partial(<function normalizer at 0x7fe400cbe820>, mean=1.1992761818593078, std=1.7452869750530826)),\n",
       " NumericColumn(key='CLNO', shape=(1,), default_value=None, dtype=tf.float64, normalizer_fn=functools.partial(<function normalizer at 0x7fe400cbe820>, mean=21.32208127284455, std=10.11116196010631)),\n",
       " NumericColumn(key='DEBTINC', shape=(1,), default_value=None, dtype=tf.float64, normalizer_fn=functools.partial(<function normalizer at 0x7fe400cbe820>, mean=33.76880402048964, std=8.806655753842648)),\n",
       " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='REASON', vocabulary_list=('DebtCon', 'HomeImp', 'Missing'), dtype=tf.string, default_value=-1, num_oov_buckets=0)),\n",
       " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='JOB', vocabulary_list=('Other', 'Sales', 'ProfExe', 'Office', 'Mgr', 'Self', 'Missing'), dtype=tf.string, default_value=-1, num_oov_buckets=0))]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {}\n",
    "for feature in feature_columns:\n",
    "    inputs = {**inputs, **tf.feature_column.make_parse_example_spec([feature])}\n",
    "    serving_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn({**inputs, **tf.feature_column.make_parse_example_spec([feature])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Value passed to parameter 'dense_defaults' has DataType float64 not in list of allowed values: float32, int64, string",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-2437ccf66f78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEXPORT_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserving_input_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mexport_saved_model\u001b[0;34m(self, export_dir_base, serving_input_receiver_fn, assets_extra, as_text, checkpoint_path, experimental_mode)\u001b[0m\n\u001b[1;32m    717\u001b[0m     \u001b[0minput_receiver_fn_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mexperimental_mode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mserving_input_receiver_fn\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m     return self._export_all_saved_models(\n\u001b[0m\u001b[1;32m    720\u001b[0m         \u001b[0mexport_dir_base\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0minput_receiver_fn_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_export_all_saved_models\u001b[0;34m(self, export_dir_base, input_receiver_fn_map, assets_extra, as_text, checkpoint_path, strip_default_attrs)\u001b[0m\n\u001b[1;32m    851\u001b[0m         \u001b[0msave_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0minput_receiver_fn_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 853\u001b[0;31m         self._add_meta_graph_for_mode(\n\u001b[0m\u001b[1;32m    854\u001b[0m             \u001b[0mbuilder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0minput_receiver_fn_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_add_meta_graph_for_mode\u001b[0;34m(self, builder, input_receiver_fn_map, checkpoint_path, save_variables, mode, export_tags, check_variables, strip_default_attrs)\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_random_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_random_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m       \u001b[0minput_receiver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_receiver_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m       \u001b[0;31m# Call the model_fn and collect the export_outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/export/export.py\u001b[0m in \u001b[0;36mserving_input_receiver_fn\u001b[0;34m()\u001b[0m\n\u001b[1;32m    308\u001b[0m         name='input_example_tensor')\n\u001b[1;32m    309\u001b[0m     \u001b[0mreceiver_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'examples'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mserialized_tf_example\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m     features = tf.compat.v1.io.parse_example(serialized_tf_example,\n\u001b[0m\u001b[1;32m    311\u001b[0m                                              feature_spec)\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mServingInputReceiver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreceiver_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/ops/parsing_ops.py\u001b[0m in \u001b[0;36mparse_example\u001b[0;34m(serialized, features, name, example_names)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_dispatch_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mparse_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mparse_example_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/ops/parsing_ops.py\u001b[0m in \u001b[0;36mparse_example_v2\u001b[0;34m(serialized, features, example_names, name)\u001b[0m\n\u001b[1;32m    312\u001b[0m   ])\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parse_example_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0m_construct_tensors_for_composite_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/ops/parsing_ops.py\u001b[0m in \u001b[0;36m_parse_example_raw\u001b[0;34m(serialized, names, params, name)\u001b[0m\n\u001b[1;32m    348\u001b[0m       raise ValueError(\"serialized must have statically-known rank to \"\n\u001b[1;32m    349\u001b[0m                        \"parse ragged features.\")\n\u001b[0;32m--> 350\u001b[0;31m     outputs = gen_parsing_ops.parse_example_v2(\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0mserialized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserialized\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/ops/gen_parsing_ops.py\u001b[0m in \u001b[0;36mparse_example_v2\u001b[0;34m(serialized, names, sparse_keys, dense_keys, ragged_keys, dense_defaults, num_sparse, sparse_types, ragged_value_types, ragged_split_types, dense_shapes, name)\u001b[0m\n\u001b[1;32m    768\u001b[0m         \"'parse_example_v2' Op, not %r.\" % dense_shapes)\n\u001b[1;32m    769\u001b[0m   \u001b[0mdense_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dense_shapes\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_s\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdense_shapes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0m\u001b[1;32m    771\u001b[0m         \u001b[0;34m\"ParseExampleV2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserialized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m                           \u001b[0msparse_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msparse_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdense_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mbase_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbase_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m             _SatisfiesTypeConstraint(base_type,\n\u001b[0m\u001b[1;32m    597\u001b[0m                                      \u001b[0m_Attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_list_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m                                      param_name=input_name)\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_SatisfiesTypeConstraint\u001b[0;34m(dtype, attr_def, param_name)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mallowed_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallowed_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallowed_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m       raise TypeError(\n\u001b[0m\u001b[1;32m     58\u001b[0m           \u001b[0;34m\"Value passed to parameter '%s' has DataType %s not in list of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m           \u001b[0;34m\"allowed values: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Value passed to parameter 'dense_defaults' has DataType float64 not in list of allowed values: float32, int64, string"
     ]
    }
   ],
   "source": [
    "estimator.export_saved_model(EXPORT_PATH, serving_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n",
    "  tf.feature_column.make_parse_example_spec([input_column]))\n",
    "estimator_base_path = os.path.join(tmpdir, 'from_estimator')\n",
    "estimator_path = estimator.export_saved_model(estimator_base_path, serving_input_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
